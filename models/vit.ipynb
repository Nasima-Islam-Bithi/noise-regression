{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe67aa54-1c7b-4aa6-9866-b0f3f0553707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import glob\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import summary as summary_lib\n",
    "from PIL import Image  # Add this import\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4097e3-c728-45d7-bff0-0800322f9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f358aa3-487a-4b2f-aced-2f2cbdf78112",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee1c317-9c12-4c6c-8ee5-d63cfdb66a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bffe6d04-06af-46cb-9ebd-4417dbd3c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sigma = 0\n",
    "max_sigma = 50/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b7906c-cd8d-473d-a4bc-17b0bc857233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image, sigma):\n",
    "    noise = torch.randn_like(image) * sigma\n",
    "    noisy_image = torch.clamp(image + noise, 0, 1)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fca37b-b513-4659-8f94-1c44a667a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_noisy_image(img, name):\n",
    "    img = img.view(img.size(0), 3, 224, 224)\n",
    "    torchvision.utils.save_image(img, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa487c6-1e4a-47a0-9f23-437d797e387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, sigma_values, transform=None, device=torch.device(\"cpu\")):\n",
    "        self.image_paths = glob.glob(os.path.join(image_folder, \"*.png\"))  # Adjust this for your image format\n",
    "        self.sigma_values = sigma_values\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sigma_values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)  # Convert NumPy array to PIL Image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sigma = torch.tensor(self.sigma_values[index], dtype=torch.float32, device=self.device)  # Ensure sigma is float32\n",
    "        return image.to(self.device), sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0777fc8-af37-4366-a6fa-6de0dd41715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customFunction():\n",
    "    sigma_values = []\n",
    "    i = 1\n",
    "    j = 1\n",
    "    for inputs, _ in train_loader:\n",
    "        sigma = np.random.uniform(min_sigma, max_sigma)\n",
    "        sigma_values.append(sigma)\n",
    "        noisy_inputs = add_noise(inputs, sigma)\n",
    "        noisy_image = (noisy_inputs.squeeze().numpy() * 255).astype(np.uint8)\n",
    "        if i <= 100:\n",
    "            new_image = torch.cat((inputs, noisy_inputs), 0)\n",
    "            save_noisy_image(new_image, f\"C:/Users/IICT3/Bithi/Noise regression/Output100/{j}.png\")\n",
    "        i += 1\n",
    "        save_noisy_image(noisy_inputs, f\"C:/Users/IICT3/Bithi/Noise regression/Noisy image/{j}.png\")\n",
    "        j += 1\n",
    "    sigma_values = np.array(sigma_values)\n",
    "    return sigma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8fdaf1f-7aaf-42dd-8f49-b1397b4bcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_values = customFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01821495-6f52-40d2-a837-be0797ea5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/IICT3/Bithi/Noise regression/Noisy image\"\n",
    "dataset = CustomDataset(image_folder=input_dir, sigma_values=sigma_values, transform=transform, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd2385a-bb0f-4225-a90c-e55a8638d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2197b4-e7b8-4d15-bb2e-aaad8289e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets_batch) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device).float()  # Convert inputs to float32\n",
    "        targets_batch = targets_batch.unsqueeze(1).to(device).float()  # Convert targets to float32\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predictions.extend(outputs.cpu().detach().numpy())\n",
    "        targets.extend(targets_batch.cpu().detach().numpy())\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Train Epoch: {epoch}\\tAverage Loss: {avg_loss:.6f}')\n",
    "    mae, rmse, r2 = calculate_metrics(predictions, targets)\n",
    "    print(f'MAE: {mae:.6f}, RMSE: {rmse:.6f}, R2 Score: {r2:.6f}')\n",
    "    writer.add_scalar('Training Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Training MAE', mae, epoch)\n",
    "    writer.add_scalar('Training RMSE', rmse, epoch)\n",
    "    writer.add_scalar('Training R2 Score', r2, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e5ffe3b-0c58-4fad-958b-7010f2debeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, epoch, writer):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_batch in test_loader:\n",
    "            inputs = inputs.to(device).float()  # Convert inputs to float32\n",
    "            targets_batch = targets_batch.unsqueeze(1).to(device).float()  # Convert targets to float32\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, targets_batch).item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().detach().numpy())\n",
    "            targets.extend(targets_batch.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f'Test Epoch: {epoch}\\tAverage Loss: {avg_loss:.6f}')\n",
    "    mae, rmse, r2 = calculate_metrics(predictions, targets)\n",
    "    print(f'MAE: {mae:.6f}, RMSE: {rmse:.6f}, R2 Score: {r2:.6f}')\n",
    "    writer.add_scalar('Testing Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Testing MAE', mae, epoch)\n",
    "    writer.add_scalar('Testing RMSE', rmse, epoch)\n",
    "    writer.add_scalar('Testing R2 Score', r2, epoch)\n",
    "    #plot_predictions(predictions, targets, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a7d7c9a-775c-4c60-8211-8c3f84751e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, targets):\n",
    "    predictions = np.array(predictions).reshape(-1)\n",
    "    targets = np.array(targets).reshape(-1)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = mean_squared_error(targets, predictions, squared=False)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4235465f-5114-4b44-9785-431fd260ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, targets, epoch):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(targets, predictions, alpha=0.5)\n",
    "    plt.xlabel('Ground Truth')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'Predictions vs. Ground Truth (Epoch {epoch})')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'predictions_vs_ground_truth_epoch_{epoch}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94d3d434-882f-40d0-81f9-b92ee6a8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8996df60-8ef2-461e-b1be-4b9bc90594e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The parameter 'pretrained' is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Arguments other than a weight enum or `None` for 'weights' are deprecated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b1b4e-08fc-4023-8b37-acbb429369fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vision transformer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "class NoiseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NoiseNet, self).__init__()\n",
    "        self.model = vit_b_16(pretrained=True)\n",
    "        self.model.heads = nn.Sequential(\n",
    "            nn.Linear(self.model.heads.head.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Assuming 'device' is defined (e.g., device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "model = NoiseNet().to(device)\n",
    "\n",
    "# Example usage\n",
    "# summary(model, (3, 224, 224))  # Uncomment if you want to use torchsummary to print the model summary\n",
    "print(model)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618b558-7f7a-4d2d-a10f-dd5e8a5b78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuned ViT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "class NoiseNet(nn.Module):\n",
    "    def __init__(self, model_name_or_path='google/vit-base-patch16-224'):\n",
    "        super(NoiseNet, self).__init__()\n",
    "        self.config = ViTConfig.from_pretrained(model_name_or_path)\n",
    "        self.model = ViTModel.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.model(pixel_values=x)\n",
    "        x = outputs.last_hidden_state[:, 0, :]  # Take the [CLS] token\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NoiseNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4b473-5ba3-4e2d-9ddd-ecc2b81f027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ViT+Resnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, vit_model_name_or_path='google/vit-base-patch16-224'):\n",
    "        super(FusionModel, self).__init__()\n",
    "        # Load ViT model\n",
    "        self.vit_config = ViTConfig.from_pretrained(vit_model_name_or_path)\n",
    "        self.vit_model = ViTModel.from_pretrained(vit_model_name_or_path, config=self.vit_config)\n",
    "        # Load ResNet-50 model\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        # Modify the classifier of ResNet-50 to match the ViT output size\n",
    "        self.resnet_model.fc = nn.Sequential(\n",
    "            nn.Linear(self.resnet_model.fc.in_features, self.vit_config.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Regression layers\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(2 * self.vit_config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ViT\n",
    "        vit_outputs = self.vit_model(pixel_values=x)\n",
    "        vit_features = vit_outputs.last_hidden_state[:, 0, :]  # Take the [CLS] token\n",
    "        # Forward pass through ResNet-50\n",
    "        resnet_features = self.resnet_model(x)\n",
    "        # Concatenate features from ViT and ResNet-50\n",
    "        combined_features = torch.cat((vit_features, resnet_features), dim=1)\n",
    "        # Forward pass through regressor\n",
    "        output = self.regressor(combined_features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of the fusion model\n",
    "model = FusionModel().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "006d33a3-94a4-46bc-b9f7-d15059a2d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionModel(\n",
      "  (vit_model): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): ViTPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (vgg_model): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1000, out_features=768, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (regressor): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#ViT+Vgg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, vit_model_name_or_path='google/vit-base-patch16-224'):\n",
    "        super(FusionModel, self).__init__()\n",
    "        # Load ViT model\n",
    "        self.vit_config = ViTConfig.from_pretrained(vit_model_name_or_path)\n",
    "        self.vit_model = ViTModel.from_pretrained(vit_model_name_or_path, config=self.vit_config)\n",
    "        # Load VGG-16 model\n",
    "        self.vgg_model = models.vgg16(pretrained=True)\n",
    "        # Get the input features of the last layer of VGG-16 classifier\n",
    "        vgg_in_features = self.vgg_model.classifier[0].in_features\n",
    "        # Modify the classifier of VGG-16 to match the ViT output size\n",
    "        self.vgg_model.classifier = nn.Sequential(\n",
    "            nn.Linear(vgg_in_features,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, self.vit_config.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Regression layers\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(2 * self.vit_config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ViT\n",
    "        vit_outputs = self.vit_model(pixel_values=x)\n",
    "        vit_features = vit_outputs.last_hidden_state[:, 0, :]  # Take the [CLS] token\n",
    "        # Forward pass through VGG-16\n",
    "        vgg_features = self.vgg_model(x)\n",
    "        # Concatenate features from ViT and VGG-16\n",
    "        combined_features = torch.cat((vit_features, vgg_features), dim=1)\n",
    "        # Forward pass through regressor\n",
    "        output = self.regressor(combined_features)\n",
    "        return output\n",
    "\n",
    "# Create an instance of the fusion model\n",
    "model = FusionModel().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7108c8a-0b3a-475a-ba71-6faca1c6d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.024454\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 0.003941\n",
      "Train Epoch: 1\tAverage Loss: 0.110007\n",
      "MAE: 0.101538, RMSE: 0.331673, R2 Score: -33.919056\n",
      "Test Epoch: 1\tAverage Loss: 0.004095\n",
      "MAE: 0.053571, RMSE: 0.063978, R2 Score: -0.296234\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.003529\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 0.003933\n",
      "Train Epoch: 2\tAverage Loss: 0.004065\n",
      "MAE: 0.053390, RMSE: 0.063755, R2 Score: -0.290224\n",
      "Test Epoch: 2\tAverage Loss: 0.003599\n",
      "MAE: 0.050919, RMSE: 0.060010, R2 Score: -0.140422\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.004218\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 0.003787\n",
      "Train Epoch: 3\tAverage Loss: 0.003894\n",
      "MAE: 0.052261, RMSE: 0.062399, R2 Score: -0.235935\n",
      "Test Epoch: 3\tAverage Loss: 0.003343\n",
      "MAE: 0.049514, RMSE: 0.057869, R2 Score: -0.060508\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003858\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 0.003435\n",
      "Train Epoch: 4\tAverage Loss: 0.003787\n",
      "MAE: 0.051764, RMSE: 0.061538, R2 Score: -0.202070\n",
      "Test Epoch: 4\tAverage Loss: 0.004640\n",
      "MAE: 0.056404, RMSE: 0.068073, R2 Score: -0.467509\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003311\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 0.004299\n",
      "Train Epoch: 5\tAverage Loss: 0.003770\n",
      "MAE: 0.051478, RMSE: 0.061398, R2 Score: -0.196612\n",
      "Test Epoch: 5\tAverage Loss: 0.003632\n",
      "MAE: 0.051100, RMSE: 0.060283, R2 Score: -0.150826\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003796\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 0.003170\n",
      "Train Epoch: 6\tAverage Loss: 0.003745\n",
      "MAE: 0.051536, RMSE: 0.061193, R2 Score: -0.188624\n",
      "Test Epoch: 6\tAverage Loss: 0.004795\n",
      "MAE: 0.057204, RMSE: 0.069194, R2 Score: -0.516209\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.003297\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 0.004077\n",
      "Train Epoch: 7\tAverage Loss: 0.003669\n",
      "MAE: 0.051178, RMSE: 0.060572, R2 Score: -0.164622\n",
      "Test Epoch: 7\tAverage Loss: 0.003610\n",
      "MAE: 0.050980, RMSE: 0.060103, R2 Score: -0.143989\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.003729\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 0.003241\n",
      "Train Epoch: 8\tAverage Loss: 0.003553\n",
      "MAE: 0.050517, RMSE: 0.059607, R2 Score: -0.127814\n",
      "Test Epoch: 8\tAverage Loss: 0.003704\n",
      "MAE: 0.051497, RMSE: 0.060871, R2 Score: -0.173400\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.003673\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 0.002944\n",
      "Train Epoch: 9\tAverage Loss: 0.003573\n",
      "MAE: 0.050669, RMSE: 0.059772, R2 Score: -0.134066\n",
      "Test Epoch: 9\tAverage Loss: 0.003894\n",
      "MAE: 0.052529, RMSE: 0.062400, R2 Score: -0.233089\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.003840\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 0.003401\n",
      "Train Epoch: 10\tAverage Loss: 0.003534\n",
      "MAE: 0.050320, RMSE: 0.059449, R2 Score: -0.121859\n",
      "Test Epoch: 10\tAverage Loss: 0.003900\n",
      "MAE: 0.052561, RMSE: 0.062447, R2 Score: -0.234965\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.003982\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 0.003235\n",
      "Train Epoch: 11\tAverage Loss: 0.003501\n",
      "MAE: 0.050229, RMSE: 0.059173, R2 Score: -0.111455\n",
      "Test Epoch: 11\tAverage Loss: 0.003500\n",
      "MAE: 0.050379, RMSE: 0.059189, R2 Score: -0.109463\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.003010\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 0.003540\n",
      "Train Epoch: 12\tAverage Loss: 0.003488\n",
      "MAE: 0.050304, RMSE: 0.059058, R2 Score: -0.107136\n",
      "Test Epoch: 12\tAverage Loss: 0.003577\n",
      "MAE: 0.050801, RMSE: 0.059831, R2 Score: -0.133660\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.003518\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 0.004319\n",
      "Train Epoch: 13\tAverage Loss: 0.003425\n",
      "MAE: 0.049918, RMSE: 0.058525, R2 Score: -0.087234\n",
      "Test Epoch: 13\tAverage Loss: 0.003841\n",
      "MAE: 0.052242, RMSE: 0.061973, R2 Score: -0.216269\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.003277\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 0.003681\n",
      "Train Epoch: 14\tAverage Loss: 0.003401\n",
      "MAE: 0.049817, RMSE: 0.058318, R2 Score: -0.079558\n",
      "Test Epoch: 14\tAverage Loss: 0.003659\n",
      "MAE: 0.051249, RMSE: 0.060505, R2 Score: -0.159313\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.003010\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 0.003132\n",
      "Train Epoch: 15\tAverage Loss: 0.003369\n",
      "MAE: 0.049583, RMSE: 0.058047, R2 Score: -0.069543\n",
      "Test Epoch: 15\tAverage Loss: 0.003732\n",
      "MAE: 0.051650, RMSE: 0.061098, R2 Score: -0.182176\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.003569\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 0.003273\n",
      "Train Epoch: 16\tAverage Loss: 0.003373\n",
      "MAE: 0.049806, RMSE: 0.058080, R2 Score: -0.070772\n",
      "Test Epoch: 16\tAverage Loss: 0.003413\n",
      "MAE: 0.049893, RMSE: 0.058459, R2 Score: -0.082241\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.003600\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 0.003206\n",
      "Train Epoch: 17\tAverage Loss: 0.003330\n",
      "MAE: 0.049374, RMSE: 0.057705, R2 Score: -0.056971\n",
      "Test Epoch: 17\tAverage Loss: 0.003745\n",
      "MAE: 0.051723, RMSE: 0.061205, R2 Score: -0.186319\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.003326\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 0.003681\n",
      "Train Epoch: 18\tAverage Loss: 0.003301\n",
      "MAE: 0.049186, RMSE: 0.057453, R2 Score: -0.047757\n",
      "Test Epoch: 18\tAverage Loss: 0.003330\n",
      "MAE: 0.049439, RMSE: 0.057760, R2 Score: -0.056539\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.003620\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 0.003773\n",
      "Train Epoch: 19\tAverage Loss: 0.003286\n",
      "MAE: 0.049203, RMSE: 0.057321, R2 Score: -0.042979\n",
      "Test Epoch: 19\tAverage Loss: 0.003349\n",
      "MAE: 0.049541, RMSE: 0.057918, R2 Score: -0.062328\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.003182\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 0.002678\n",
      "Train Epoch: 20\tAverage Loss: 0.003254\n",
      "MAE: 0.048957, RMSE: 0.057044, R2 Score: -0.032900\n",
      "Test Epoch: 20\tAverage Loss: 0.003194\n",
      "MAE: 0.048688, RMSE: 0.056595, R2 Score: -0.014324\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.003298\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 0.004079\n",
      "Train Epoch: 21\tAverage Loss: 0.003265\n",
      "MAE: 0.049141, RMSE: 0.057140, R2 Score: -0.036395\n",
      "Test Epoch: 21\tAverage Loss: 0.003204\n",
      "MAE: 0.048740, RMSE: 0.056683, R2 Score: -0.017496\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.003217\n",
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 0.003515\n",
      "Train Epoch: 22\tAverage Loss: 0.003216\n",
      "MAE: 0.048712, RMSE: 0.056707, R2 Score: -0.020740\n",
      "Test Epoch: 22\tAverage Loss: 0.003184\n",
      "MAE: 0.048640, RMSE: 0.056512, R2 Score: -0.011344\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.002959\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 0.003190\n",
      "Train Epoch: 23\tAverage Loss: 0.003217\n",
      "MAE: 0.048889, RMSE: 0.056720, R2 Score: -0.021192\n",
      "Test Epoch: 23\tAverage Loss: 0.003233\n",
      "MAE: 0.048896, RMSE: 0.056930, R2 Score: -0.026375\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.002797\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 0.004121\n",
      "Train Epoch: 24\tAverage Loss: 0.003190\n",
      "MAE: 0.048633, RMSE: 0.056484, R2 Score: -0.012712\n",
      "Test Epoch: 24\tAverage Loss: 0.003157\n",
      "MAE: 0.048511, RMSE: 0.056291, R2 Score: -0.003475\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.003109\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 0.003690\n",
      "Train Epoch: 25\tAverage Loss: 0.003202\n",
      "MAE: 0.048749, RMSE: 0.056583, R2 Score: -0.016298\n",
      "Test Epoch: 25\tAverage Loss: 0.003208\n",
      "MAE: 0.048764, RMSE: 0.056721, R2 Score: -0.018852\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.003355\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 0.003496\n",
      "Train Epoch: 26\tAverage Loss: 0.003178\n",
      "MAE: 0.048541, RMSE: 0.056371, R2 Score: -0.008685\n",
      "Test Epoch: 26\tAverage Loss: 0.003250\n",
      "MAE: 0.048989, RMSE: 0.057074, R2 Score: -0.031565\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.002980\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 0.003205\n",
      "Train Epoch: 27\tAverage Loss: 0.003200\n",
      "MAE: 0.048648, RMSE: 0.056573, R2 Score: -0.015906\n",
      "Test Epoch: 27\tAverage Loss: 0.003166\n",
      "MAE: 0.048555, RMSE: 0.056366, R2 Score: -0.006127\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.003436\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 0.003131\n",
      "Train Epoch: 28\tAverage Loss: 0.003167\n",
      "MAE: 0.048480, RMSE: 0.056278, R2 Score: -0.005351\n",
      "Test Epoch: 28\tAverage Loss: 0.003147\n",
      "MAE: 0.048484, RMSE: 0.056205, R2 Score: -0.000399\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.002819\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 0.002820\n",
      "Train Epoch: 29\tAverage Loss: 0.003173\n",
      "MAE: 0.048526, RMSE: 0.056328, R2 Score: -0.007149\n",
      "Test Epoch: 29\tAverage Loss: 0.003146\n",
      "MAE: 0.048500, RMSE: 0.056204, R2 Score: -0.000352\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.003096\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 0.002903\n",
      "Train Epoch: 30\tAverage Loss: 0.003177\n",
      "MAE: 0.048639, RMSE: 0.056365, R2 Score: -0.008466\n",
      "Test Epoch: 30\tAverage Loss: 0.003148\n",
      "MAE: 0.048483, RMSE: 0.056214, R2 Score: -0.000721\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.010619\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 0.003291\n",
      "Train Epoch: 1\tAverage Loss: 0.006001\n",
      "MAE: 0.056994, RMSE: 0.077466, R2 Score: -0.912584\n",
      "Test Epoch: 1\tAverage Loss: 0.003225\n",
      "MAE: 0.049333, RMSE: 0.056833, R2 Score: -0.007815\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.004176\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 0.003280\n",
      "Train Epoch: 2\tAverage Loss: 0.003336\n",
      "MAE: 0.049455, RMSE: 0.057758, R2 Score: -0.063226\n",
      "Test Epoch: 2\tAverage Loss: 0.003315\n",
      "MAE: 0.049760, RMSE: 0.057606, R2 Score: -0.035440\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003099\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 0.003059\n",
      "Train Epoch: 3\tAverage Loss: 0.003267\n",
      "MAE: 0.049018, RMSE: 0.057154, R2 Score: -0.041090\n",
      "Test Epoch: 3\tAverage Loss: 0.003257\n",
      "MAE: 0.049495, RMSE: 0.057108, R2 Score: -0.017581\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003203\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 0.003192\n",
      "Train Epoch: 4\tAverage Loss: 0.003225\n",
      "MAE: 0.048766, RMSE: 0.056788, R2 Score: -0.027801\n",
      "Test Epoch: 4\tAverage Loss: 0.003272\n",
      "MAE: 0.049565, RMSE: 0.057238, R2 Score: -0.022242\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.002940\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 0.002820\n",
      "Train Epoch: 5\tAverage Loss: 0.003193\n",
      "MAE: 0.048574, RMSE: 0.056508, R2 Score: -0.017709\n",
      "Test Epoch: 5\tAverage Loss: 0.003369\n",
      "MAE: 0.050008, RMSE: 0.058069, R2 Score: -0.052145\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.004170\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 0.003337\n",
      "Train Epoch: 6\tAverage Loss: 0.003181\n",
      "MAE: 0.048497, RMSE: 0.056402, R2 Score: -0.013881\n",
      "Test Epoch: 6\tAverage Loss: 0.003206\n",
      "MAE: 0.049219, RMSE: 0.056672, R2 Score: -0.002117\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.003766\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 0.003190\n",
      "Train Epoch: 7\tAverage Loss: 0.003168\n",
      "MAE: 0.048487, RMSE: 0.056288, R2 Score: -0.009781\n",
      "Test Epoch: 7\tAverage Loss: 0.003297\n",
      "MAE: 0.049680, RMSE: 0.057451, R2 Score: -0.029872\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.003416\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 0.002963\n",
      "Train Epoch: 8\tAverage Loss: 0.003164\n",
      "MAE: 0.048449, RMSE: 0.056252, R2 Score: -0.008509\n",
      "Test Epoch: 8\tAverage Loss: 0.003213\n",
      "MAE: 0.049266, RMSE: 0.056731, R2 Score: -0.004211\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.003548\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 0.002887\n",
      "Train Epoch: 9\tAverage Loss: 0.003155\n",
      "MAE: 0.048370, RMSE: 0.056168, R2 Score: -0.005504\n",
      "Test Epoch: 9\tAverage Loss: 0.003221\n",
      "MAE: 0.049314, RMSE: 0.056802, R2 Score: -0.006709\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.003428\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 0.003183\n",
      "Train Epoch: 10\tAverage Loss: 0.003149\n",
      "MAE: 0.048343, RMSE: 0.056117, R2 Score: -0.003686\n",
      "Test Epoch: 10\tAverage Loss: 0.003211\n",
      "MAE: 0.049254, RMSE: 0.056714, R2 Score: -0.003600\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.003556\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 0.003148\n",
      "Train Epoch: 11\tAverage Loss: 0.003145\n",
      "MAE: 0.048291, RMSE: 0.056078, R2 Score: -0.002278\n",
      "Test Epoch: 11\tAverage Loss: 0.003223\n",
      "MAE: 0.049322, RMSE: 0.056815, R2 Score: -0.007193\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.002438\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 0.003276\n",
      "Train Epoch: 12\tAverage Loss: 0.003145\n",
      "MAE: 0.048325, RMSE: 0.056079, R2 Score: -0.002302\n",
      "Test Epoch: 12\tAverage Loss: 0.003223\n",
      "MAE: 0.049322, RMSE: 0.056815, R2 Score: -0.007193\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.003634\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 0.002941\n",
      "Train Epoch: 13\tAverage Loss: 0.003148\n",
      "MAE: 0.048319, RMSE: 0.056107, R2 Score: -0.003302\n",
      "Test Epoch: 13\tAverage Loss: 0.003213\n",
      "MAE: 0.049270, RMSE: 0.056737, R2 Score: -0.004404\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.003478\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 0.003488\n",
      "Train Epoch: 14\tAverage Loss: 0.003146\n",
      "MAE: 0.048320, RMSE: 0.056086, R2 Score: -0.002556\n",
      "Test Epoch: 14\tAverage Loss: 0.003215\n",
      "MAE: 0.049281, RMSE: 0.056751, R2 Score: -0.004931\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.002972\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 0.002454\n",
      "Train Epoch: 15\tAverage Loss: 0.003143\n",
      "MAE: 0.048312, RMSE: 0.056062, R2 Score: -0.001709\n",
      "Test Epoch: 15\tAverage Loss: 0.003211\n",
      "MAE: 0.049259, RMSE: 0.056720, R2 Score: -0.003835\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.003901\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 0.003550\n",
      "Train Epoch: 16\tAverage Loss: 0.003141\n",
      "MAE: 0.048272, RMSE: 0.056048, R2 Score: -0.001214\n",
      "Test Epoch: 16\tAverage Loss: 0.003202\n",
      "MAE: 0.049185, RMSE: 0.056638, R2 Score: -0.000925\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.002738\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 0.002829\n",
      "Train Epoch: 17\tAverage Loss: 0.003143\n",
      "MAE: 0.048304, RMSE: 0.056058, R2 Score: -0.001563\n",
      "Test Epoch: 17\tAverage Loss: 0.003202\n",
      "MAE: 0.049187, RMSE: 0.056640, R2 Score: -0.001005\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.002832\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 0.002971\n",
      "Train Epoch: 18\tAverage Loss: 0.003143\n",
      "MAE: 0.048306, RMSE: 0.056063, R2 Score: -0.001728\n",
      "Test Epoch: 18\tAverage Loss: 0.003198\n",
      "MAE: 0.049149, RMSE: 0.056612, R2 Score: 0.000014\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.002818\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 0.002792\n",
      "Train Epoch: 19\tAverage Loss: 0.003145\n",
      "MAE: 0.048297, RMSE: 0.056080, R2 Score: -0.002356\n",
      "Test Epoch: 19\tAverage Loss: 0.003231\n",
      "MAE: 0.049366, RMSE: 0.056887, R2 Score: -0.009736\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.002462\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 0.003357\n",
      "Train Epoch: 20\tAverage Loss: 0.003143\n",
      "MAE: 0.048289, RMSE: 0.056067, R2 Score: -0.001869\n",
      "Test Epoch: 20\tAverage Loss: 0.003222\n",
      "MAE: 0.049320, RMSE: 0.056812, R2 Score: -0.007071\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.003524\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 0.003494\n",
      "Train Epoch: 21\tAverage Loss: 0.003142\n",
      "MAE: 0.048290, RMSE: 0.056056, R2 Score: -0.001507\n",
      "Test Epoch: 21\tAverage Loss: 0.003215\n",
      "MAE: 0.049280, RMSE: 0.056749, R2 Score: -0.004858\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.003113\n",
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 0.002717\n",
      "Train Epoch: 22\tAverage Loss: 0.003151\n",
      "MAE: 0.048347, RMSE: 0.056133, R2 Score: -0.004226\n",
      "Test Epoch: 22\tAverage Loss: 0.003224\n",
      "MAE: 0.049329, RMSE: 0.056826, R2 Score: -0.007574\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.003256\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 0.003180\n",
      "Train Epoch: 23\tAverage Loss: 0.003144\n",
      "MAE: 0.048317, RMSE: 0.056067, R2 Score: -0.001899\n",
      "Test Epoch: 23\tAverage Loss: 0.003203\n",
      "MAE: 0.049201, RMSE: 0.056654, R2 Score: -0.001482\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.003281\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 0.003208\n",
      "Train Epoch: 24\tAverage Loss: 0.003142\n",
      "MAE: 0.048306, RMSE: 0.056051, R2 Score: -0.001307\n",
      "Test Epoch: 24\tAverage Loss: 0.003210\n",
      "MAE: 0.049250, RMSE: 0.056709, R2 Score: -0.003445\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.003392\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 0.003661\n",
      "Train Epoch: 25\tAverage Loss: 0.003140\n",
      "MAE: 0.048286, RMSE: 0.056036, R2 Score: -0.000779\n",
      "Test Epoch: 25\tAverage Loss: 0.003200\n",
      "MAE: 0.049138, RMSE: 0.056631, R2 Score: -0.000686\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.004034\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 0.002375\n",
      "Train Epoch: 26\tAverage Loss: 0.003142\n",
      "MAE: 0.048283, RMSE: 0.056054, R2 Score: -0.001432\n",
      "Test Epoch: 26\tAverage Loss: 0.003203\n",
      "MAE: 0.049198, RMSE: 0.056650, R2 Score: -0.001347\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.003555\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 0.002621\n",
      "Train Epoch: 27\tAverage Loss: 0.003143\n",
      "MAE: 0.048316, RMSE: 0.056063, R2 Score: -0.001746\n",
      "Test Epoch: 27\tAverage Loss: 0.003200\n",
      "MAE: 0.049172, RMSE: 0.056626, R2 Score: -0.000506\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.003671\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 0.003258\n",
      "Train Epoch: 28\tAverage Loss: 0.003143\n",
      "MAE: 0.048310, RMSE: 0.056061, R2 Score: -0.001650\n",
      "Test Epoch: 28\tAverage Loss: 0.003202\n",
      "MAE: 0.049187, RMSE: 0.056640, R2 Score: -0.000987\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.003666\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 0.002489\n",
      "Train Epoch: 29\tAverage Loss: 0.003141\n",
      "MAE: 0.048302, RMSE: 0.056046, R2 Score: -0.001118\n",
      "Test Epoch: 29\tAverage Loss: 0.003199\n",
      "MAE: 0.049166, RMSE: 0.056621, R2 Score: -0.000332\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.003447\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 0.003261\n",
      "Train Epoch: 30\tAverage Loss: 0.003144\n",
      "MAE: 0.048310, RMSE: 0.056071, R2 Score: -0.002039\n",
      "Test Epoch: 30\tAverage Loss: 0.003206\n",
      "MAE: 0.049219, RMSE: 0.056672, R2 Score: -0.002124\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.035047\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 0.003012\n",
      "Train Epoch: 1\tAverage Loss: 0.047399\n",
      "MAE: 0.071928, RMSE: 0.217713, R2 Score: -13.943551\n",
      "Test Epoch: 1\tAverage Loss: 0.003121\n",
      "MAE: 0.048150, RMSE: 0.056021, R2 Score: -0.022671\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.003407\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 0.003512\n",
      "Train Epoch: 2\tAverage Loss: 0.003374\n",
      "MAE: 0.049761, RMSE: 0.058090, R2 Score: -0.063850\n",
      "Test Epoch: 2\tAverage Loss: 0.003118\n",
      "MAE: 0.048132, RMSE: 0.055992, R2 Score: -0.021626\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003366\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 0.003541\n",
      "Train Epoch: 3\tAverage Loss: 0.003307\n",
      "MAE: 0.049300, RMSE: 0.057507, R2 Score: -0.042615\n",
      "Test Epoch: 3\tAverage Loss: 0.003051\n",
      "MAE: 0.047745, RMSE: 0.055407, R2 Score: -0.000371\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003803\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 0.002649\n",
      "Train Epoch: 4\tAverage Loss: 0.003300\n",
      "MAE: 0.049308, RMSE: 0.057446, R2 Score: -0.040427\n",
      "Test Epoch: 4\tAverage Loss: 0.003081\n",
      "MAE: 0.047931, RMSE: 0.055668, R2 Score: -0.009817\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003490\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 0.002604\n",
      "Train Epoch: 5\tAverage Loss: 0.003285\n",
      "MAE: 0.049257, RMSE: 0.057311, R2 Score: -0.035521\n",
      "Test Epoch: 5\tAverage Loss: 0.003057\n",
      "MAE: 0.047787, RMSE: 0.055455, R2 Score: -0.002113\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.002811\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 0.003575\n",
      "Train Epoch: 6\tAverage Loss: 0.003241\n",
      "MAE: 0.048972, RMSE: 0.056931, R2 Score: -0.021826\n",
      "Test Epoch: 6\tAverage Loss: 0.003053\n",
      "MAE: 0.047761, RMSE: 0.055422, R2 Score: -0.000919\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.002315\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 0.003089\n",
      "Train Epoch: 7\tAverage Loss: 0.003229\n",
      "MAE: 0.048909, RMSE: 0.056827, R2 Score: -0.018094\n",
      "Test Epoch: 7\tAverage Loss: 0.003050\n",
      "MAE: 0.047739, RMSE: 0.055394, R2 Score: 0.000105\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.003864\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 0.003184\n",
      "Train Epoch: 8\tAverage Loss: 0.003223\n",
      "MAE: 0.048913, RMSE: 0.056768, R2 Score: -0.016004\n",
      "Test Epoch: 8\tAverage Loss: 0.003052\n",
      "MAE: 0.047750, RMSE: 0.055417, R2 Score: -0.000746\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.003045\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 0.003340\n",
      "Train Epoch: 9\tAverage Loss: 0.003212\n",
      "MAE: 0.048737, RMSE: 0.056670, R2 Score: -0.012506\n",
      "Test Epoch: 9\tAverage Loss: 0.003051\n",
      "MAE: 0.047743, RMSE: 0.055403, R2 Score: -0.000237\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.003077\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 0.003004\n",
      "Train Epoch: 10\tAverage Loss: 0.003208\n",
      "MAE: 0.048833, RMSE: 0.056638, R2 Score: -0.011356\n",
      "Test Epoch: 10\tAverage Loss: 0.003070\n",
      "MAE: 0.047864, RMSE: 0.055566, R2 Score: -0.006133\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.003500\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 0.003781\n",
      "Train Epoch: 11\tAverage Loss: 0.003210\n",
      "MAE: 0.048850, RMSE: 0.056660, R2 Score: -0.012128\n",
      "Test Epoch: 11\tAverage Loss: 0.003051\n",
      "MAE: 0.047745, RMSE: 0.055402, R2 Score: -0.000201\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.002831\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 0.002871\n",
      "Train Epoch: 12\tAverage Loss: 0.003191\n",
      "MAE: 0.048687, RMSE: 0.056488, R2 Score: -0.006006\n",
      "Test Epoch: 12\tAverage Loss: 0.003051\n",
      "MAE: 0.047745, RMSE: 0.055401, R2 Score: -0.000172\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.002899\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 0.002846\n",
      "Train Epoch: 13\tAverage Loss: 0.003199\n",
      "MAE: 0.048827, RMSE: 0.056560, R2 Score: -0.008550\n",
      "Test Epoch: 13\tAverage Loss: 0.003052\n",
      "MAE: 0.047749, RMSE: 0.055415, R2 Score: -0.000669\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.002453\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 0.002679\n",
      "Train Epoch: 14\tAverage Loss: 0.003196\n",
      "MAE: 0.048766, RMSE: 0.056537, R2 Score: -0.007755\n",
      "Test Epoch: 14\tAverage Loss: 0.003052\n",
      "MAE: 0.047751, RMSE: 0.055418, R2 Score: -0.000778\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.003923\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 0.003090\n",
      "Train Epoch: 15\tAverage Loss: 0.003192\n",
      "MAE: 0.048701, RMSE: 0.056499, R2 Score: -0.006379\n",
      "Test Epoch: 15\tAverage Loss: 0.003061\n",
      "MAE: 0.047798, RMSE: 0.055499, R2 Score: -0.003697\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.003612\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 0.003163\n",
      "Train Epoch: 16\tAverage Loss: 0.003182\n",
      "MAE: 0.048660, RMSE: 0.056408, R2 Score: -0.003168\n",
      "Test Epoch: 16\tAverage Loss: 0.003064\n",
      "MAE: 0.047830, RMSE: 0.055515, R2 Score: -0.004300\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.003354\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 0.002376\n",
      "Train Epoch: 17\tAverage Loss: 0.003195\n",
      "MAE: 0.048784, RMSE: 0.056523, R2 Score: -0.007236\n",
      "Test Epoch: 17\tAverage Loss: 0.003062\n",
      "MAE: 0.047805, RMSE: 0.055509, R2 Score: -0.004078\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.003618\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 0.003655\n",
      "Train Epoch: 18\tAverage Loss: 0.003186\n",
      "MAE: 0.048695, RMSE: 0.056442, R2 Score: -0.004378\n",
      "Test Epoch: 18\tAverage Loss: 0.003052\n",
      "MAE: 0.047749, RMSE: 0.055415, R2 Score: -0.000675\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.003288\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 0.002667\n",
      "Train Epoch: 19\tAverage Loss: 0.003185\n",
      "MAE: 0.048687, RMSE: 0.056434, R2 Score: -0.004067\n",
      "Test Epoch: 19\tAverage Loss: 0.003056\n",
      "MAE: 0.047778, RMSE: 0.055443, R2 Score: -0.001666\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.003872\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 0.003017\n",
      "Train Epoch: 20\tAverage Loss: 0.003180\n",
      "MAE: 0.048665, RMSE: 0.056388, R2 Score: -0.002432\n",
      "Test Epoch: 20\tAverage Loss: 0.003051\n",
      "MAE: 0.047744, RMSE: 0.055404, R2 Score: -0.000280\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.002479\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 0.003190\n",
      "Train Epoch: 21\tAverage Loss: 0.003182\n",
      "MAE: 0.048673, RMSE: 0.056407, R2 Score: -0.003115\n",
      "Test Epoch: 21\tAverage Loss: 0.003054\n",
      "MAE: 0.047760, RMSE: 0.055435, R2 Score: -0.001377\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.003012\n",
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 0.002658\n",
      "Train Epoch: 22\tAverage Loss: 0.003175\n",
      "MAE: 0.048643, RMSE: 0.056347, R2 Score: -0.000999\n",
      "Test Epoch: 22\tAverage Loss: 0.003057\n",
      "MAE: 0.047774, RMSE: 0.055459, R2 Score: -0.002256\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.003604\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 0.003485\n",
      "Train Epoch: 23\tAverage Loss: 0.003179\n",
      "MAE: 0.048660, RMSE: 0.056382, R2 Score: -0.002238\n",
      "Test Epoch: 23\tAverage Loss: 0.003054\n",
      "MAE: 0.047759, RMSE: 0.055433, R2 Score: -0.001325\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.003547\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 0.003464\n",
      "Train Epoch: 24\tAverage Loss: 0.003176\n",
      "MAE: 0.048642, RMSE: 0.056358, R2 Score: -0.001369\n",
      "Test Epoch: 24\tAverage Loss: 0.003051\n",
      "MAE: 0.047746, RMSE: 0.055403, R2 Score: -0.000230\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.003367\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 0.003290\n",
      "Train Epoch: 25\tAverage Loss: 0.003182\n",
      "MAE: 0.048686, RMSE: 0.056414, R2 Score: -0.003351\n",
      "Test Epoch: 25\tAverage Loss: 0.003064\n",
      "MAE: 0.047817, RMSE: 0.055527, R2 Score: -0.004733\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.003413\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 0.003269\n",
      "Train Epoch: 26\tAverage Loss: 0.003180\n",
      "MAE: 0.048671, RMSE: 0.056390, R2 Score: -0.002509\n",
      "Test Epoch: 26\tAverage Loss: 0.003053\n",
      "MAE: 0.047751, RMSE: 0.055419, R2 Score: -0.000823\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.003192\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 0.003075\n",
      "Train Epoch: 27\tAverage Loss: 0.003179\n",
      "MAE: 0.048680, RMSE: 0.056382, R2 Score: -0.002218\n",
      "Test Epoch: 27\tAverage Loss: 0.003051\n",
      "MAE: 0.047743, RMSE: 0.055404, R2 Score: -0.000260\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.003149\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 0.002823\n",
      "Train Epoch: 28\tAverage Loss: 0.003179\n",
      "MAE: 0.048679, RMSE: 0.056381, R2 Score: -0.002191\n",
      "Test Epoch: 28\tAverage Loss: 0.003052\n",
      "MAE: 0.047748, RMSE: 0.055414, R2 Score: -0.000631\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.003293\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 0.003053\n",
      "Train Epoch: 29\tAverage Loss: 0.003174\n",
      "MAE: 0.048639, RMSE: 0.056336, R2 Score: -0.000593\n",
      "Test Epoch: 29\tAverage Loss: 0.003052\n",
      "MAE: 0.047749, RMSE: 0.055416, R2 Score: -0.000690\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.003508\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 0.003084\n",
      "Train Epoch: 30\tAverage Loss: 0.003173\n",
      "MAE: 0.048622, RMSE: 0.056328, R2 Score: -0.000321\n",
      "Test Epoch: 30\tAverage Loss: 0.003055\n",
      "MAE: 0.047765, RMSE: 0.055444, R2 Score: -0.001709\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.010497\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 0.003251\n",
      "Train Epoch: 1\tAverage Loss: 0.008699\n",
      "MAE: 0.058752, RMSE: 0.093267, R2 Score: -1.778663\n",
      "Test Epoch: 1\tAverage Loss: 0.003247\n",
      "MAE: 0.049130, RMSE: 0.057010, R2 Score: -0.004072\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.002821\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 0.003001\n",
      "Train Epoch: 2\tAverage Loss: 0.003352\n",
      "MAE: 0.049543, RMSE: 0.057899, R2 Score: -0.070821\n",
      "Test Epoch: 2\tAverage Loss: 0.003287\n",
      "MAE: 0.049433, RMSE: 0.057372, R2 Score: -0.016838\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003788\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 0.003186\n",
      "Train Epoch: 3\tAverage Loss: 0.003316\n",
      "MAE: 0.049299, RMSE: 0.057589, R2 Score: -0.059380\n",
      "Test Epoch: 3\tAverage Loss: 0.003372\n",
      "MAE: 0.049873, RMSE: 0.058108, R2 Score: -0.043095\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003486\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 0.003114\n",
      "Train Epoch: 4\tAverage Loss: 0.003247\n",
      "MAE: 0.048924, RMSE: 0.056984, R2 Score: -0.037236\n",
      "Test Epoch: 4\tAverage Loss: 0.003248\n",
      "MAE: 0.049213, RMSE: 0.057029, R2 Score: -0.004730\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003561\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 0.002390\n",
      "Train Epoch: 5\tAverage Loss: 0.003233\n",
      "MAE: 0.048792, RMSE: 0.056858, R2 Score: -0.032662\n",
      "Test Epoch: 5\tAverage Loss: 0.003234\n",
      "MAE: 0.049111, RMSE: 0.056903, R2 Score: -0.000283\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003276\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 0.003295\n",
      "Train Epoch: 6\tAverage Loss: 0.003195\n",
      "MAE: 0.048606, RMSE: 0.056526, R2 Score: -0.020642\n",
      "Test Epoch: 6\tAverage Loss: 0.003281\n",
      "MAE: 0.049401, RMSE: 0.057316, R2 Score: -0.014858\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.003220\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 0.003853\n",
      "Train Epoch: 7\tAverage Loss: 0.003184\n",
      "MAE: 0.048588, RMSE: 0.056425, R2 Score: -0.016989\n",
      "Test Epoch: 7\tAverage Loss: 0.003233\n",
      "MAE: 0.049093, RMSE: 0.056894, R2 Score: 0.000008\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002954\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 0.003318\n",
      "Train Epoch: 8\tAverage Loss: 0.003168\n",
      "MAE: 0.048509, RMSE: 0.056284, R2 Score: -0.011938\n",
      "Test Epoch: 8\tAverage Loss: 0.003238\n",
      "MAE: 0.049141, RMSE: 0.056939, R2 Score: -0.001543\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.003256\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 0.003335\n",
      "Train Epoch: 9\tAverage Loss: 0.003154\n",
      "MAE: 0.048380, RMSE: 0.056156, R2 Score: -0.007334\n",
      "Test Epoch: 9\tAverage Loss: 0.003234\n",
      "MAE: 0.049089, RMSE: 0.056897, R2 Score: -0.000094\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.003081\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 0.003030\n",
      "Train Epoch: 10\tAverage Loss: 0.003147\n",
      "MAE: 0.048360, RMSE: 0.056098, R2 Score: -0.005245\n",
      "Test Epoch: 10\tAverage Loss: 0.003237\n",
      "MAE: 0.049133, RMSE: 0.056928, R2 Score: -0.001188\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.003155\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 0.003483\n",
      "Train Epoch: 11\tAverage Loss: 0.003164\n",
      "MAE: 0.048434, RMSE: 0.056246, R2 Score: -0.010551\n",
      "Test Epoch: 11\tAverage Loss: 0.003256\n",
      "MAE: 0.049178, RMSE: 0.057089, R2 Score: -0.006837\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.003081\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 0.004263\n",
      "Train Epoch: 12\tAverage Loss: 0.003162\n",
      "MAE: 0.048469, RMSE: 0.056230, R2 Score: -0.009966\n",
      "Test Epoch: 12\tAverage Loss: 0.003236\n",
      "MAE: 0.049088, RMSE: 0.056915, R2 Score: -0.000731\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.003045\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 0.003300\n",
      "Train Epoch: 13\tAverage Loss: 0.003145\n",
      "MAE: 0.048377, RMSE: 0.056082, R2 Score: -0.004675\n",
      "Test Epoch: 13\tAverage Loss: 0.003236\n",
      "MAE: 0.049089, RMSE: 0.056921, R2 Score: -0.000916\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.003277\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 0.003288\n",
      "Train Epoch: 14\tAverage Loss: 0.003136\n",
      "MAE: 0.048306, RMSE: 0.056002, R2 Score: -0.001791\n",
      "Test Epoch: 14\tAverage Loss: 0.003233\n",
      "MAE: 0.049099, RMSE: 0.056897, R2 Score: -0.000076\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.003108\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 0.002923\n",
      "Train Epoch: 15\tAverage Loss: 0.003140\n",
      "MAE: 0.048348, RMSE: 0.056039, R2 Score: -0.003115\n",
      "Test Epoch: 15\tAverage Loss: 0.003238\n",
      "MAE: 0.049095, RMSE: 0.056939, R2 Score: -0.001546\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.003163\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 0.003030\n",
      "Train Epoch: 16\tAverage Loss: 0.003138\n",
      "MAE: 0.048318, RMSE: 0.056015, R2 Score: -0.002261\n",
      "Test Epoch: 16\tAverage Loss: 0.003239\n",
      "MAE: 0.049096, RMSE: 0.056941, R2 Score: -0.001637\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.003265\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 0.003398\n",
      "Train Epoch: 17\tAverage Loss: 0.003139\n",
      "MAE: 0.048353, RMSE: 0.056022, R2 Score: -0.002536\n",
      "Test Epoch: 17\tAverage Loss: 0.003233\n",
      "MAE: 0.049090, RMSE: 0.056896, R2 Score: -0.000040\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.003026\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 0.003179\n",
      "Train Epoch: 18\tAverage Loss: 0.003140\n",
      "MAE: 0.048365, RMSE: 0.056034, R2 Score: -0.002953\n",
      "Test Epoch: 18\tAverage Loss: 0.003237\n",
      "MAE: 0.049091, RMSE: 0.056926, R2 Score: -0.001118\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.003104\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 0.003868\n",
      "Train Epoch: 19\tAverage Loss: 0.003136\n",
      "MAE: 0.048331, RMSE: 0.056000, R2 Score: -0.001744\n",
      "Test Epoch: 19\tAverage Loss: 0.003233\n",
      "MAE: 0.049091, RMSE: 0.056895, R2 Score: -0.000015\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.003330\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 0.003430\n",
      "Train Epoch: 20\tAverage Loss: 0.003136\n",
      "MAE: 0.048321, RMSE: 0.056000, R2 Score: -0.001733\n",
      "Test Epoch: 20\tAverage Loss: 0.003236\n",
      "MAE: 0.049122, RMSE: 0.056916, R2 Score: -0.000738\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.002569\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 0.003594\n",
      "Train Epoch: 21\tAverage Loss: 0.003138\n",
      "MAE: 0.048345, RMSE: 0.056020, R2 Score: -0.002451\n",
      "Test Epoch: 21\tAverage Loss: 0.003238\n",
      "MAE: 0.049094, RMSE: 0.056936, R2 Score: -0.001439\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.003360\n",
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 0.002508\n",
      "Train Epoch: 22\tAverage Loss: 0.003136\n",
      "MAE: 0.048321, RMSE: 0.055996, R2 Score: -0.001597\n",
      "Test Epoch: 22\tAverage Loss: 0.003236\n",
      "MAE: 0.049126, RMSE: 0.056921, R2 Score: -0.000917\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.003293\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 0.002856\n",
      "Train Epoch: 23\tAverage Loss: 0.003136\n",
      "MAE: 0.048332, RMSE: 0.056003, R2 Score: -0.001844\n",
      "Test Epoch: 23\tAverage Loss: 0.003236\n",
      "MAE: 0.049088, RMSE: 0.056914, R2 Score: -0.000666\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.002811\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 0.003441\n",
      "Train Epoch: 24\tAverage Loss: 0.003138\n",
      "MAE: 0.048320, RMSE: 0.056019, R2 Score: -0.002406\n",
      "Test Epoch: 24\tAverage Loss: 0.003238\n",
      "MAE: 0.049093, RMSE: 0.056934, R2 Score: -0.001381\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.002775\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 0.002967\n",
      "Train Epoch: 25\tAverage Loss: 0.003134\n",
      "MAE: 0.048306, RMSE: 0.055981, R2 Score: -0.001059\n",
      "Test Epoch: 25\tAverage Loss: 0.003233\n",
      "MAE: 0.049098, RMSE: 0.056896, R2 Score: -0.000051\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.003322\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 0.002788\n",
      "Train Epoch: 26\tAverage Loss: 0.003133\n",
      "MAE: 0.048303, RMSE: 0.055971, R2 Score: -0.000687\n",
      "Test Epoch: 26\tAverage Loss: 0.003237\n",
      "MAE: 0.049090, RMSE: 0.056922, R2 Score: -0.000961\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.003035\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 0.003283\n",
      "Train Epoch: 27\tAverage Loss: 0.003133\n",
      "MAE: 0.048311, RMSE: 0.055974, R2 Score: -0.000817\n",
      "Test Epoch: 27\tAverage Loss: 0.003245\n",
      "MAE: 0.049124, RMSE: 0.056998, R2 Score: -0.003651\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.003457\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 0.002649\n",
      "Train Epoch: 28\tAverage Loss: 0.003135\n",
      "MAE: 0.048334, RMSE: 0.055992, R2 Score: -0.001438\n",
      "Test Epoch: 28\tAverage Loss: 0.003233\n",
      "MAE: 0.049090, RMSE: 0.056896, R2 Score: -0.000038\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.003557\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 0.002768\n",
      "Train Epoch: 29\tAverage Loss: 0.003134\n",
      "MAE: 0.048304, RMSE: 0.055985, R2 Score: -0.001208\n",
      "Test Epoch: 29\tAverage Loss: 0.003234\n",
      "MAE: 0.049102, RMSE: 0.056899, R2 Score: -0.000146\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.003100\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 0.002942\n",
      "Train Epoch: 30\tAverage Loss: 0.003135\n",
      "MAE: 0.048316, RMSE: 0.055994, R2 Score: -0.001509\n",
      "Test Epoch: 30\tAverage Loss: 0.003248\n",
      "MAE: 0.049136, RMSE: 0.057019, R2 Score: -0.004369\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.041783\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 0.002918\n",
      "Train Epoch: 1\tAverage Loss: 0.023524\n",
      "MAE: 0.071539, RMSE: 0.153376, R2 Score: -6.423790\n",
      "Test Epoch: 1\tAverage Loss: 0.003393\n",
      "MAE: 0.049447, RMSE: 0.058359, R2 Score: -0.104211\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.003153\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 0.004483\n",
      "Train Epoch: 2\tAverage Loss: 0.003827\n",
      "MAE: 0.052072, RMSE: 0.061863, R2 Score: -0.207726\n",
      "Test Epoch: 2\tAverage Loss: 0.003087\n",
      "MAE: 0.047857, RMSE: 0.055764, R2 Score: -0.008173\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003954\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 0.004290\n",
      "Train Epoch: 3\tAverage Loss: 0.003694\n",
      "MAE: 0.051348, RMSE: 0.060777, R2 Score: -0.165697\n",
      "Test Epoch: 3\tAverage Loss: 0.003428\n",
      "MAE: 0.049636, RMSE: 0.058645, R2 Score: -0.115048\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003794\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 0.003214\n",
      "Train Epoch: 4\tAverage Loss: 0.003588\n",
      "MAE: 0.050922, RMSE: 0.059897, R2 Score: -0.132174\n",
      "Test Epoch: 4\tAverage Loss: 0.003112\n",
      "MAE: 0.047971, RMSE: 0.055970, R2 Score: -0.015639\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003476\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 0.003753\n",
      "Train Epoch: 5\tAverage Loss: 0.003589\n",
      "MAE: 0.050816, RMSE: 0.059912, R2 Score: -0.132760\n",
      "Test Epoch: 5\tAverage Loss: 0.003411\n",
      "MAE: 0.049542, RMSE: 0.058504, R2 Score: -0.109693\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003417\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 0.003201\n",
      "Train Epoch: 6\tAverage Loss: 0.003516\n",
      "MAE: 0.050502, RMSE: 0.059295, R2 Score: -0.109565\n",
      "Test Epoch: 6\tAverage Loss: 0.003121\n",
      "MAE: 0.048015, RMSE: 0.056050, R2 Score: -0.018547\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.003099\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 0.003382\n",
      "Train Epoch: 7\tAverage Loss: 0.003425\n",
      "MAE: 0.049853, RMSE: 0.058520, R2 Score: -0.080739\n",
      "Test Epoch: 7\tAverage Loss: 0.003083\n",
      "MAE: 0.047835, RMSE: 0.055732, R2 Score: -0.007020\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002928\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 0.004129\n",
      "Train Epoch: 8\tAverage Loss: 0.003399\n",
      "MAE: 0.049807, RMSE: 0.058303, R2 Score: -0.072724\n",
      "Test Epoch: 8\tAverage Loss: 0.003424\n",
      "MAE: 0.049614, RMSE: 0.058616, R2 Score: -0.113957\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.003123\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 0.002922\n",
      "Train Epoch: 9\tAverage Loss: 0.003337\n",
      "MAE: 0.049543, RMSE: 0.057765, R2 Score: -0.053034\n",
      "Test Epoch: 9\tAverage Loss: 0.003192\n",
      "MAE: 0.048375, RMSE: 0.056656, R2 Score: -0.040684\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.002696\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 0.002764\n",
      "Train Epoch: 10\tAverage Loss: 0.003314\n",
      "MAE: 0.049370, RMSE: 0.057563, R2 Score: -0.045688\n",
      "Test Epoch: 10\tAverage Loss: 0.003082\n",
      "MAE: 0.047830, RMSE: 0.055723, R2 Score: -0.006715\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.003347\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 0.003498\n",
      "Train Epoch: 11\tAverage Loss: 0.003279\n",
      "MAE: 0.049157, RMSE: 0.057266, R2 Score: -0.034920\n",
      "Test Epoch: 11\tAverage Loss: 0.003157\n",
      "MAE: 0.048197, RMSE: 0.056358, R2 Score: -0.029774\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.003150\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 0.003231\n",
      "Train Epoch: 12\tAverage Loss: 0.003246\n",
      "MAE: 0.048984, RMSE: 0.056973, R2 Score: -0.024353\n",
      "Test Epoch: 12\tAverage Loss: 0.003149\n",
      "MAE: 0.048155, RMSE: 0.056287, R2 Score: -0.027205\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.003083\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 0.003114\n",
      "Train Epoch: 13\tAverage Loss: 0.003244\n",
      "MAE: 0.048893, RMSE: 0.056955, R2 Score: -0.023717\n",
      "Test Epoch: 13\tAverage Loss: 0.003058\n",
      "MAE: 0.047768, RMSE: 0.055536, R2 Score: 0.000021\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.002995\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 0.004143\n",
      "Train Epoch: 14\tAverage Loss: 0.003258\n",
      "MAE: 0.049176, RMSE: 0.057081, R2 Score: -0.028237\n",
      "Test Epoch: 14\tAverage Loss: 0.003077\n",
      "MAE: 0.047811, RMSE: 0.055680, R2 Score: -0.005139\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.003503\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 0.003546\n",
      "Train Epoch: 15\tAverage Loss: 0.003206\n",
      "MAE: 0.048778, RMSE: 0.056626, R2 Score: -0.011894\n",
      "Test Epoch: 15\tAverage Loss: 0.003059\n",
      "MAE: 0.047765, RMSE: 0.055544, R2 Score: -0.000250\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.003703\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 0.003159\n",
      "Train Epoch: 16\tAverage Loss: 0.003207\n",
      "MAE: 0.048874, RMSE: 0.056632, R2 Score: -0.012126\n",
      "Test Epoch: 16\tAverage Loss: 0.003068\n",
      "MAE: 0.047781, RMSE: 0.055608, R2 Score: -0.002569\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.002759\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 0.003373\n",
      "Train Epoch: 17\tAverage Loss: 0.003217\n",
      "MAE: 0.048891, RMSE: 0.056715, R2 Score: -0.015081\n",
      "Test Epoch: 17\tAverage Loss: 0.003089\n",
      "MAE: 0.047864, RMSE: 0.055778, R2 Score: -0.008687\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.003353\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 0.003227\n",
      "Train Epoch: 18\tAverage Loss: 0.003203\n",
      "MAE: 0.048830, RMSE: 0.056597, R2 Score: -0.010872\n",
      "Test Epoch: 18\tAverage Loss: 0.003070\n",
      "MAE: 0.047882, RMSE: 0.055669, R2 Score: -0.004765\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.002810\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 0.002686\n",
      "Train Epoch: 19\tAverage Loss: 0.003198\n",
      "MAE: 0.048809, RMSE: 0.056551, R2 Score: -0.009220\n",
      "Test Epoch: 19\tAverage Loss: 0.003060\n",
      "MAE: 0.047805, RMSE: 0.055570, R2 Score: -0.001202\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.003516\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 0.002861\n",
      "Train Epoch: 20\tAverage Loss: 0.003185\n",
      "MAE: 0.048695, RMSE: 0.056436, R2 Score: -0.005139\n",
      "Test Epoch: 20\tAverage Loss: 0.003062\n",
      "MAE: 0.047824, RMSE: 0.055593, R2 Score: -0.002013\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.002790\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 0.003610\n",
      "Train Epoch: 21\tAverage Loss: 0.003203\n",
      "MAE: 0.048838, RMSE: 0.056595, R2 Score: -0.010787\n",
      "Test Epoch: 21\tAverage Loss: 0.003068\n",
      "MAE: 0.047780, RMSE: 0.055605, R2 Score: -0.002462\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.002965\n",
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 0.003004\n",
      "Train Epoch: 22\tAverage Loss: 0.003177\n",
      "MAE: 0.048656, RMSE: 0.056368, R2 Score: -0.002699\n",
      "Test Epoch: 22\tAverage Loss: 0.003060\n",
      "MAE: 0.047761, RMSE: 0.055544, R2 Score: -0.000258\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.003247\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 0.002885\n",
      "Train Epoch: 23\tAverage Loss: 0.003181\n",
      "MAE: 0.048680, RMSE: 0.056398, R2 Score: -0.003778\n",
      "Test Epoch: 23\tAverage Loss: 0.003073\n",
      "MAE: 0.047904, RMSE: 0.055697, R2 Score: -0.005778\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.003246\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 0.003101\n",
      "Train Epoch: 24\tAverage Loss: 0.003187\n",
      "MAE: 0.048752, RMSE: 0.056451, R2 Score: -0.005667\n",
      "Test Epoch: 24\tAverage Loss: 0.003059\n",
      "MAE: 0.047762, RMSE: 0.055542, R2 Score: -0.000163\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.003812\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 0.003319\n",
      "Train Epoch: 25\tAverage Loss: 0.003178\n",
      "MAE: 0.048689, RMSE: 0.056376, R2 Score: -0.002995\n",
      "Test Epoch: 25\tAverage Loss: 0.003059\n",
      "MAE: 0.047797, RMSE: 0.055560, R2 Score: -0.000838\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.003341\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 0.003409\n",
      "Train Epoch: 26\tAverage Loss: 0.003184\n",
      "MAE: 0.048730, RMSE: 0.056427, R2 Score: -0.004825\n",
      "Test Epoch: 26\tAverage Loss: 0.003058\n",
      "MAE: 0.047769, RMSE: 0.055537, R2 Score: -0.000009\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.003050\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 0.002722\n",
      "Train Epoch: 27\tAverage Loss: 0.003175\n",
      "MAE: 0.048631, RMSE: 0.056343, R2 Score: -0.001821\n",
      "Test Epoch: 27\tAverage Loss: 0.003058\n",
      "MAE: 0.047780, RMSE: 0.055544, R2 Score: -0.000245\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.003271\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 0.003086\n",
      "Train Epoch: 28\tAverage Loss: 0.003178\n",
      "MAE: 0.048677, RMSE: 0.056370, R2 Score: -0.002785\n",
      "Test Epoch: 28\tAverage Loss: 0.003058\n",
      "MAE: 0.047771, RMSE: 0.055538, R2 Score: -0.000028\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.003193\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 0.002920\n",
      "Train Epoch: 29\tAverage Loss: 0.003180\n",
      "MAE: 0.048698, RMSE: 0.056395, R2 Score: -0.003666\n",
      "Test Epoch: 29\tAverage Loss: 0.003073\n",
      "MAE: 0.047799, RMSE: 0.055650, R2 Score: -0.004068\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.003465\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 0.003463\n",
      "Train Epoch: 30\tAverage Loss: 0.003179\n",
      "MAE: 0.048693, RMSE: 0.056384, R2 Score: -0.003273\n",
      "Test Epoch: 30\tAverage Loss: 0.003061\n",
      "MAE: 0.047812, RMSE: 0.055578, R2 Score: -0.001472\n"
     ]
    }
   ],
   "source": [
    "#Trainig\n",
    "\n",
    "from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "# Initialize empty lists to store metrics\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Training and testing loop with cross-validation\n",
    "epochs = 30\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    trainloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = FusionModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    custom_run_name = f\"ViT+Vgg_fold_{fold + 1}\"\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{custom_run_name}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, trainloader, loss_fn, optimizer, epoch, writer)\n",
    "        test(model, testloader, loss_fn, epoch, writer)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Read scalar values from tensorboard event files\n",
    "\n",
    "    event_file = glob.glob(f\"runs/{custom_run_name}/events.out.tfevents.*\")[0]\n",
    "    event_loader = EventFileLoader(event_file)\n",
    "    scalar_events = [event for event in event_loader.Load()]\n",
    "\n",
    "\n",
    "    # Extract the required scalar values\n",
    "    train_loss = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 1), None)\n",
    "    test_loss = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 2), None)\n",
    "    mae = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 3), None)\n",
    "    rmse = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 4), None)\n",
    "    r2 = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 5), None)\n",
    "\n",
    "\n",
    "    # Append the metrics to the respective lists\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6bed6-fe48-4282-bffd-4b8e63fe9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ViT_VGG.pth')\n",
    "model.load_state_dict(torch.load('ViT_VGG.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031974fb-97c3-4863-9016-496e7119ac1d",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac79d5-f887-4403-849a-180e105289f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sigma = 0\n",
    "max_sigma = 50/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2106d-69ef-4c2b-8813-c980d9f8833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image, sigma):\n",
    "    noise = torch.randn_like(image) * sigma\n",
    "    noisy_image = torch.clamp(image + noise, 0, 1)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b977fcb-71e8-4d97-bc4b-585253cd3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382cab4-a4e8-43fa-afa3-5fa98bee5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images with actual and predicted sigma values\n",
    "def display_images(original, noisy_images, actual_sigmas, predicted_sigmas):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(len(noisy_images)):\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(np.transpose(noisy_images[i].cpu().numpy(), (1, 2, 0)))\n",
    "        ax.set_title(f\"Actual Sigma: {actual_sigmas[i]:.4f}, Predicted Sigma: {predicted_sigmas[i]:.4f}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(np.transpose(original.cpu().numpy(), (1, 2, 0)))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c78672-809f-4849-baf8-ec9d258172fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to apply noise and predict sigma\n",
    "def main(image_path):\n",
    "    original_image = load_image(image_path).to(device)\n",
    "    sigmas = np.linspace(min_sigma,max_sigma, 5)  # Define 5 different sigma values\n",
    "    noisy_images = [add_noise(original_image, sigma) for sigma in sigmas]\n",
    "    \n",
    "    # Predict sigma values\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_sigmas = []\n",
    "        for noisy_image in noisy_images:\n",
    "            output = model(noisy_image)\n",
    "            predicted_sigma = output.item()\n",
    "            predicted_sigmas.append(predicted_sigma)\n",
    "\n",
    "    # Display the results\n",
    "    display_images(original_image.squeeze(0), [ni.squeeze(0) for ni in noisy_images], sigmas, predicted_sigmas)\n",
    "\n",
    "# Run the main function with the path to your image\n",
    "main('C:/Users/IICT3/Bithi/Noise regression/bird.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6754c2c6-0880-4fa9-a79f-034d4eb99065",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c0ae483-9a36-4df7-8fdc-51f24aa15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd5de5-a44a-4079-864f-bd8caabc7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713034f-08bc-4fd4-af41-f21bb4cdca61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "logs_base_dir = \"./runs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836598d5-82a6-4a27-8390-00174d519295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
