{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe67aa54-1c7b-4aa6-9866-b0f3f0553707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import glob\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import summary as summary_lib\n",
    "from PIL import Image  # Add this import\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4097e3-c728-45d7-bff0-0800322f9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f358aa3-487a-4b2f-aced-2f2cbdf78112",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((227, 227)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee1c317-9c12-4c6c-8ee5-d63cfdb66a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bffe6d04-06af-46cb-9ebd-4417dbd3c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sigma = 0\n",
    "max_sigma = 50/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b7906c-cd8d-473d-a4bc-17b0bc857233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image, sigma):\n",
    "    noise = torch.randn_like(image) * sigma\n",
    "    noisy_image = torch.clamp(image + noise, 0, 1)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fca37b-b513-4659-8f94-1c44a667a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_noisy_image(img, name):\n",
    "    img = img.view(img.size(0), 3, 227, 227)\n",
    "    torchvision.utils.save_image(img, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa487c6-1e4a-47a0-9f23-437d797e387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, sigma_values, transform=None, device=torch.device(\"cpu\")):\n",
    "        self.image_paths = glob.glob(os.path.join(image_folder, \"*.png\"))  # Adjust this for your image format\n",
    "        self.sigma_values = sigma_values\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sigma_values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)  # Convert NumPy array to PIL Image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sigma = torch.tensor(self.sigma_values[index], dtype=torch.float32, device=self.device)  # Ensure sigma is float32\n",
    "        return image.to(self.device), sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0777fc8-af37-4366-a6fa-6de0dd41715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customFunction():\n",
    "    sigma_values = []\n",
    "    i = 1\n",
    "    j = 1\n",
    "    for inputs, _ in train_loader:\n",
    "        sigma = np.random.uniform(min_sigma, max_sigma)\n",
    "        sigma_values.append(sigma)\n",
    "        noisy_inputs = add_noise(inputs, sigma)\n",
    "        noisy_image = (noisy_inputs.squeeze().numpy() * 255).astype(np.uint8)\n",
    "        if i <= 100:\n",
    "            new_image = torch.cat((inputs, noisy_inputs), 0)\n",
    "            save_noisy_image(new_image, f\"C:/Users/IICT3/Bithi/Noise regression/Output100/{j}.png\")\n",
    "        i += 1\n",
    "        save_noisy_image(noisy_inputs, f\"C:/Users/IICT3/Bithi/Noise regression/Noisy image/{j}.png\")\n",
    "        j += 1\n",
    "    sigma_values = np.array(sigma_values)\n",
    "    return sigma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fdaf1f-7aaf-42dd-8f49-b1397b4bcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_values = customFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01821495-6f52-40d2-a837-be0797ea5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/IICT3/Bithi/Noise regression/Noisy image\"\n",
    "dataset = CustomDataset(image_folder=input_dir, sigma_values=sigma_values, transform=transform, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd2385a-bb0f-4225-a90c-e55a8638d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2197b4-e7b8-4d15-bb2e-aaad8289e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets_batch) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device).float()  # Convert inputs to float32\n",
    "        targets_batch = targets_batch.unsqueeze(1).to(device).float()  # Convert targets to float32\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predictions.extend(outputs.cpu().detach().numpy())\n",
    "        targets.extend(targets_batch.cpu().detach().numpy())\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Train Epoch: {epoch}\\tAverage Loss: {avg_loss:.6f}')\n",
    "    mae, rmse, r2 = calculate_metrics(predictions, targets)\n",
    "    print(f'MAE: {mae:.6f}, RMSE: {rmse:.6f}, R2 Score: {r2:.6f}')\n",
    "    writer.add_scalar('Training Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Training MAE', mae, epoch)\n",
    "    writer.add_scalar('Training RMSE', rmse, epoch)\n",
    "    writer.add_scalar('Training R2 Score', r2, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5ffe3b-0c58-4fad-958b-7010f2debeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, epoch, writer):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_batch in test_loader:\n",
    "            inputs = inputs.to(device).float()  # Convert inputs to float32\n",
    "            targets_batch = targets_batch.unsqueeze(1).to(device).float()  # Convert targets to float32\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, targets_batch).item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().detach().numpy())\n",
    "            targets.extend(targets_batch.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f'Test Epoch: {epoch}\\tAverage Loss: {avg_loss:.6f}')\n",
    "    mae, rmse, r2 = calculate_metrics(predictions, targets)\n",
    "    print(f'MAE: {mae:.6f}, RMSE: {rmse:.6f}, R2 Score: {r2:.6f}')\n",
    "    writer.add_scalar('Testing Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Testing MAE', mae, epoch)\n",
    "    writer.add_scalar('Testing RMSE', rmse, epoch)\n",
    "    writer.add_scalar('Testing R2 Score', r2, epoch)\n",
    "    #plot_predictions(predictions, targets, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7d7c9a-775c-4c60-8211-8c3f84751e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, targets):\n",
    "    predictions = np.array(predictions).reshape(-1)\n",
    "    targets = np.array(targets).reshape(-1)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = mean_squared_error(targets, predictions, squared=False)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4235465f-5114-4b44-9785-431fd260ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, targets, epoch):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(targets, predictions, alpha=0.5)\n",
    "    plt.xlabel('Ground Truth')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'Predictions vs. Ground Truth (Epoch {epoch})')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'predictions_vs_ground_truth_epoch_{epoch}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d3d434-882f-40d0-81f9-b92ee6a8259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8996df60-8ef2-461e-b1be-4b9bc90594e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The parameter 'pretrained' is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Arguments other than a weight enum or `None` for 'weights' are deprecated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "200b1b4e-08fc-4023-8b37-acbb429369fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoiseNet(\n",
       "  (model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.25, inplace=False)\n",
       "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Dropout(p=0.5, inplace=False)\n",
       "      (7): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Efficientnet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NoiseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NoiseNet, self).__init__()\n",
    "        self.model = models.efficientnet_b0(pretrained=True)\n",
    "        # Modify the classifier for regression\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280,512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.25),\n",
    "                                  nn.Linear(512,256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.5),\n",
    "                                  nn.Linear(256, 1)\n",
    "                            \n",
    ")\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = NoiseNet().to(device)\n",
    "#summary(model1,(3,227,227))\n",
    "\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7108c8a-0b3a-475a-ba71-6faca1c6d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.006365\n",
      "Train Epoch: 1\tAverage Loss: 0.004784\n",
      "MAE: 0.054589, RMSE: 0.069279, R2 Score: -0.517451\n",
      "Test Epoch: 1\tAverage Loss: 0.003314\n",
      "MAE: 0.049785, RMSE: 0.057467, R2 Score: -0.022033\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.003338\n",
      "Train Epoch: 2\tAverage Loss: 0.003319\n",
      "MAE: 0.049343, RMSE: 0.057582, R2 Score: -0.048290\n",
      "Test Epoch: 2\tAverage Loss: 0.003352\n",
      "MAE: 0.050142, RMSE: 0.057848, R2 Score: -0.035606\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003569\n",
      "Train Epoch: 3\tAverage Loss: 0.003279\n",
      "MAE: 0.049011, RMSE: 0.057188, R2 Score: -0.034002\n",
      "Test Epoch: 3\tAverage Loss: 0.003333\n",
      "MAE: 0.049842, RMSE: 0.057618, R2 Score: -0.027393\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003342\n",
      "Train Epoch: 4\tAverage Loss: 0.003244\n",
      "MAE: 0.048824, RMSE: 0.056922, R2 Score: -0.024399\n",
      "Test Epoch: 4\tAverage Loss: 0.003294\n",
      "MAE: 0.049659, RMSE: 0.057288, R2 Score: -0.015653\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003196\n",
      "Train Epoch: 5\tAverage Loss: 0.003182\n",
      "MAE: 0.048660, RMSE: 0.056465, R2 Score: -0.008015\n",
      "Test Epoch: 5\tAverage Loss: 0.003274\n",
      "MAE: 0.049588, RMSE: 0.057125, R2 Score: -0.009901\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003040\n",
      "Train Epoch: 6\tAverage Loss: 0.003135\n",
      "MAE: 0.048063, RMSE: 0.055922, R2 Score: 0.011270\n",
      "Test Epoch: 6\tAverage Loss: 0.003319\n",
      "MAE: 0.049766, RMSE: 0.057530, R2 Score: -0.024269\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.002917\n",
      "Train Epoch: 7\tAverage Loss: 0.003084\n",
      "MAE: 0.047667, RMSE: 0.055501, R2 Score: 0.026099\n",
      "Test Epoch: 7\tAverage Loss: 0.003400\n",
      "MAE: 0.050285, RMSE: 0.058220, R2 Score: -0.048991\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002561\n",
      "Train Epoch: 8\tAverage Loss: 0.002977\n",
      "MAE: 0.046647, RMSE: 0.054569, R2 Score: 0.058532\n",
      "Test Epoch: 8\tAverage Loss: 0.003512\n",
      "MAE: 0.050645, RMSE: 0.059192, R2 Score: -0.084287\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.002882\n",
      "Train Epoch: 9\tAverage Loss: 0.002809\n",
      "MAE: 0.044814, RMSE: 0.052966, R2 Score: 0.113033\n",
      "Test Epoch: 9\tAverage Loss: 0.003415\n",
      "MAE: 0.050262, RMSE: 0.058397, R2 Score: -0.055364\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.002530\n",
      "Train Epoch: 10\tAverage Loss: 0.002575\n",
      "MAE: 0.042230, RMSE: 0.050763, R2 Score: 0.185305\n",
      "Test Epoch: 10\tAverage Loss: 0.003476\n",
      "MAE: 0.050744, RMSE: 0.058864, R2 Score: -0.072293\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.002385\n",
      "Train Epoch: 11\tAverage Loss: 0.002243\n",
      "MAE: 0.039150, RMSE: 0.047371, R2 Score: 0.290539\n",
      "Test Epoch: 11\tAverage Loss: 0.003969\n",
      "MAE: 0.052910, RMSE: 0.062906, R2 Score: -0.224630\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.001671\n",
      "Train Epoch: 12\tAverage Loss: 0.001858\n",
      "MAE: 0.035118, RMSE: 0.043080, R2 Score: 0.413241\n",
      "Test Epoch: 12\tAverage Loss: 0.004147\n",
      "MAE: 0.053700, RMSE: 0.064296, R2 Score: -0.279367\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.001436\n",
      "Train Epoch: 13\tAverage Loss: 0.001496\n",
      "MAE: 0.031069, RMSE: 0.038659, R2 Score: 0.527482\n",
      "Test Epoch: 13\tAverage Loss: 0.004312\n",
      "MAE: 0.054955, RMSE: 0.065635, R2 Score: -0.333200\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.001101\n",
      "Train Epoch: 14\tAverage Loss: 0.001125\n",
      "MAE: 0.026743, RMSE: 0.033492, R2 Score: 0.645365\n",
      "Test Epoch: 14\tAverage Loss: 0.004448\n",
      "MAE: 0.055335, RMSE: 0.066616, R2 Score: -0.373357\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.001021\n",
      "Train Epoch: 15\tAverage Loss: 0.000974\n",
      "MAE: 0.024862, RMSE: 0.031220, R2 Score: 0.691844\n",
      "Test Epoch: 15\tAverage Loss: 0.004448\n",
      "MAE: 0.055221, RMSE: 0.066660, R2 Score: -0.375157\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.000909\n",
      "Train Epoch: 16\tAverage Loss: 0.000792\n",
      "MAE: 0.022419, RMSE: 0.028157, R2 Score: 0.749333\n",
      "Test Epoch: 16\tAverage Loss: 0.004259\n",
      "MAE: 0.054570, RMSE: 0.065255, R2 Score: -0.317781\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.000733\n",
      "Train Epoch: 17\tAverage Loss: 0.000687\n",
      "MAE: 0.020806, RMSE: 0.026235, R2 Score: 0.782390\n",
      "Test Epoch: 17\tAverage Loss: 0.004221\n",
      "MAE: 0.054088, RMSE: 0.064928, R2 Score: -0.304623\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.000578\n",
      "Train Epoch: 18\tAverage Loss: 0.000596\n",
      "MAE: 0.019309, RMSE: 0.024421, R2 Score: 0.811453\n",
      "Test Epoch: 18\tAverage Loss: 0.004234\n",
      "MAE: 0.054608, RMSE: 0.064985, R2 Score: -0.306925\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.000395\n",
      "Train Epoch: 19\tAverage Loss: 0.000536\n",
      "MAE: 0.018268, RMSE: 0.023121, R2 Score: 0.830988\n",
      "Test Epoch: 19\tAverage Loss: 0.003985\n",
      "MAE: 0.053230, RMSE: 0.063092, R2 Score: -0.231879\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.000557\n",
      "Train Epoch: 20\tAverage Loss: 0.000543\n",
      "MAE: 0.018383, RMSE: 0.023310, R2 Score: 0.828212\n",
      "Test Epoch: 20\tAverage Loss: 0.004358\n",
      "MAE: 0.055338, RMSE: 0.066018, R2 Score: -0.348777\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.000582\n",
      "Train Epoch: 21\tAverage Loss: 0.000492\n",
      "MAE: 0.017350, RMSE: 0.022218, R2 Score: 0.843936\n",
      "Test Epoch: 21\tAverage Loss: 0.004059\n",
      "MAE: 0.053290, RMSE: 0.063683, R2 Score: -0.255069\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.000426\n",
      "Train Epoch: 22\tAverage Loss: 0.000486\n",
      "MAE: 0.017413, RMSE: 0.022044, R2 Score: 0.846367\n",
      "Test Epoch: 22\tAverage Loss: 0.004263\n",
      "MAE: 0.054450, RMSE: 0.065269, R2 Score: -0.318372\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.000383\n",
      "Train Epoch: 23\tAverage Loss: 0.000465\n",
      "MAE: 0.017007, RMSE: 0.021557, R2 Score: 0.853073\n",
      "Test Epoch: 23\tAverage Loss: 0.004145\n",
      "MAE: 0.053901, RMSE: 0.064362, R2 Score: -0.281979\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.000447\n",
      "Train Epoch: 24\tAverage Loss: 0.000412\n",
      "MAE: 0.015914, RMSE: 0.020280, R2 Score: 0.869968\n",
      "Test Epoch: 24\tAverage Loss: 0.003900\n",
      "MAE: 0.052448, RMSE: 0.062441, R2 Score: -0.206581\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.000451\n",
      "Train Epoch: 25\tAverage Loss: 0.000398\n",
      "MAE: 0.015704, RMSE: 0.019935, R2 Score: 0.874353\n",
      "Test Epoch: 25\tAverage Loss: 0.004043\n",
      "MAE: 0.053546, RMSE: 0.063607, R2 Score: -0.252088\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.000321\n",
      "Train Epoch: 26\tAverage Loss: 0.000372\n",
      "MAE: 0.015190, RMSE: 0.019271, R2 Score: 0.882581\n",
      "Test Epoch: 26\tAverage Loss: 0.003955\n",
      "MAE: 0.053032, RMSE: 0.062863, R2 Score: -0.222938\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.000336\n",
      "Train Epoch: 27\tAverage Loss: 0.000347\n",
      "MAE: 0.014774, RMSE: 0.018624, R2 Score: 0.890338\n",
      "Test Epoch: 27\tAverage Loss: 0.004118\n",
      "MAE: 0.053798, RMSE: 0.064130, R2 Score: -0.272743\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.000394\n",
      "Train Epoch: 28\tAverage Loss: 0.000335\n",
      "MAE: 0.014352, RMSE: 0.018289, R2 Score: 0.894243\n",
      "Test Epoch: 28\tAverage Loss: 0.003848\n",
      "MAE: 0.052335, RMSE: 0.061927, R2 Score: -0.186812\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.000358\n",
      "Train Epoch: 29\tAverage Loss: 0.000329\n",
      "MAE: 0.014280, RMSE: 0.018156, R2 Score: 0.895781\n",
      "Test Epoch: 29\tAverage Loss: 0.003932\n",
      "MAE: 0.052857, RMSE: 0.062610, R2 Score: -0.213150\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.000269\n",
      "Train Epoch: 30\tAverage Loss: 0.000329\n",
      "MAE: 0.014306, RMSE: 0.018117, R2 Score: 0.896230\n",
      "Test Epoch: 30\tAverage Loss: 0.004149\n",
      "MAE: 0.054005, RMSE: 0.064358, R2 Score: -0.281805\n",
      "Fold 2\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.007449\n",
      "Train Epoch: 1\tAverage Loss: 0.006582\n",
      "MAE: 0.059546, RMSE: 0.081281, R2 Score: -1.084762\n",
      "Test Epoch: 1\tAverage Loss: 0.003699\n",
      "MAE: 0.051103, RMSE: 0.060737, R2 Score: -0.149462\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.003609\n",
      "Train Epoch: 2\tAverage Loss: 0.003523\n",
      "MAE: 0.050340, RMSE: 0.059355, R2 Score: -0.111697\n",
      "Test Epoch: 2\tAverage Loss: 0.003311\n",
      "MAE: 0.048958, RMSE: 0.057465, R2 Score: -0.028968\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003248\n",
      "Train Epoch: 3\tAverage Loss: 0.003386\n",
      "MAE: 0.049431, RMSE: 0.058192, R2 Score: -0.068549\n",
      "Test Epoch: 3\tAverage Loss: 0.003398\n",
      "MAE: 0.049621, RMSE: 0.058239, R2 Score: -0.056851\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003291\n",
      "Train Epoch: 4\tAverage Loss: 0.003296\n",
      "MAE: 0.049018, RMSE: 0.057399, R2 Score: -0.039642\n",
      "Test Epoch: 4\tAverage Loss: 0.003470\n",
      "MAE: 0.049831, RMSE: 0.058837, R2 Score: -0.078688\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003250\n",
      "Train Epoch: 5\tAverage Loss: 0.003149\n",
      "MAE: 0.047438, RMSE: 0.056074, R2 Score: 0.007808\n",
      "Test Epoch: 5\tAverage Loss: 0.003383\n",
      "MAE: 0.049776, RMSE: 0.058171, R2 Score: -0.054392\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003513\n",
      "Train Epoch: 6\tAverage Loss: 0.002927\n",
      "MAE: 0.045737, RMSE: 0.054100, R2 Score: 0.076425\n",
      "Test Epoch: 6\tAverage Loss: 0.003660\n",
      "MAE: 0.051370, RMSE: 0.060519, R2 Score: -0.141251\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.002679\n",
      "Train Epoch: 7\tAverage Loss: 0.002591\n",
      "MAE: 0.042516, RMSE: 0.050953, R2 Score: 0.180753\n",
      "Test Epoch: 7\tAverage Loss: 0.003928\n",
      "MAE: 0.052672, RMSE: 0.062622, R2 Score: -0.221938\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002118\n",
      "Train Epoch: 8\tAverage Loss: 0.002175\n",
      "MAE: 0.038345, RMSE: 0.046667, R2 Score: 0.312782\n",
      "Test Epoch: 8\tAverage Loss: 0.004041\n",
      "MAE: 0.053287, RMSE: 0.063611, R2 Score: -0.260834\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.001892\n",
      "Train Epoch: 9\tAverage Loss: 0.001807\n",
      "MAE: 0.034313, RMSE: 0.042479, R2 Score: 0.430583\n",
      "Test Epoch: 9\tAverage Loss: 0.004169\n",
      "MAE: 0.053840, RMSE: 0.064578, R2 Score: -0.299439\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.001305\n",
      "Train Epoch: 10\tAverage Loss: 0.001504\n",
      "MAE: 0.031153, RMSE: 0.038798, R2 Score: 0.524990\n",
      "Test Epoch: 10\tAverage Loss: 0.004030\n",
      "MAE: 0.053131, RMSE: 0.063509, R2 Score: -0.256808\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.001157\n",
      "Train Epoch: 11\tAverage Loss: 0.001290\n",
      "MAE: 0.028581, RMSE: 0.035906, R2 Score: 0.593180\n",
      "Test Epoch: 11\tAverage Loss: 0.004002\n",
      "MAE: 0.052990, RMSE: 0.063250, R2 Score: -0.246570\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.001116\n",
      "Train Epoch: 12\tAverage Loss: 0.001052\n",
      "MAE: 0.025695, RMSE: 0.032443, R2 Score: 0.667862\n",
      "Test Epoch: 12\tAverage Loss: 0.004165\n",
      "MAE: 0.054118, RMSE: 0.064573, R2 Score: -0.299264\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.000888\n",
      "Train Epoch: 13\tAverage Loss: 0.000895\n",
      "MAE: 0.023466, RMSE: 0.029922, R2 Score: 0.717477\n",
      "Test Epoch: 13\tAverage Loss: 0.004243\n",
      "MAE: 0.054513, RMSE: 0.065147, R2 Score: -0.322448\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.000722\n",
      "Train Epoch: 14\tAverage Loss: 0.000781\n",
      "MAE: 0.021972, RMSE: 0.027933, R2 Score: 0.753785\n",
      "Test Epoch: 14\tAverage Loss: 0.004264\n",
      "MAE: 0.054268, RMSE: 0.065241, R2 Score: -0.326291\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.000561\n",
      "Train Epoch: 15\tAverage Loss: 0.000752\n",
      "MAE: 0.021629, RMSE: 0.027369, R2 Score: 0.763630\n",
      "Test Epoch: 15\tAverage Loss: 0.003870\n",
      "MAE: 0.052300, RMSE: 0.062300, R2 Score: -0.209402\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.000672\n",
      "Train Epoch: 16\tAverage Loss: 0.000658\n",
      "MAE: 0.020232, RMSE: 0.025662, R2 Score: 0.792202\n",
      "Test Epoch: 16\tAverage Loss: 0.004069\n",
      "MAE: 0.053412, RMSE: 0.063839, R2 Score: -0.269867\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.000508\n",
      "Train Epoch: 17\tAverage Loss: 0.000591\n",
      "MAE: 0.019178, RMSE: 0.024304, R2 Score: 0.813614\n",
      "Test Epoch: 17\tAverage Loss: 0.003897\n",
      "MAE: 0.052241, RMSE: 0.062463, R2 Score: -0.215741\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.000350\n",
      "Train Epoch: 18\tAverage Loss: 0.000533\n",
      "MAE: 0.018216, RMSE: 0.023053, R2 Score: 0.832303\n",
      "Test Epoch: 18\tAverage Loss: 0.003870\n",
      "MAE: 0.052222, RMSE: 0.062238, R2 Score: -0.206988\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.000519\n",
      "Train Epoch: 19\tAverage Loss: 0.000522\n",
      "MAE: 0.018017, RMSE: 0.022846, R2 Score: 0.835301\n",
      "Test Epoch: 19\tAverage Loss: 0.003849\n",
      "MAE: 0.052158, RMSE: 0.062064, R2 Score: -0.200262\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.000434\n",
      "Train Epoch: 20\tAverage Loss: 0.000457\n",
      "MAE: 0.016882, RMSE: 0.021374, R2 Score: 0.855839\n",
      "Test Epoch: 20\tAverage Loss: 0.004149\n",
      "MAE: 0.053990, RMSE: 0.064452, R2 Score: -0.294408\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.000396\n",
      "Train Epoch: 21\tAverage Loss: 0.000448\n",
      "MAE: 0.016628, RMSE: 0.021147, R2 Score: 0.858883\n",
      "Test Epoch: 21\tAverage Loss: 0.003907\n",
      "MAE: 0.052471, RMSE: 0.062486, R2 Score: -0.216645\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.000507\n",
      "Train Epoch: 22\tAverage Loss: 0.000402\n",
      "MAE: 0.015694, RMSE: 0.020050, R2 Score: 0.873143\n",
      "Test Epoch: 22\tAverage Loss: 0.003988\n",
      "MAE: 0.053051, RMSE: 0.063150, R2 Score: -0.242603\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.000378\n",
      "Train Epoch: 23\tAverage Loss: 0.000416\n",
      "MAE: 0.015952, RMSE: 0.020371, R2 Score: 0.869055\n",
      "Test Epoch: 23\tAverage Loss: 0.003774\n",
      "MAE: 0.051910, RMSE: 0.061460, R2 Score: -0.176992\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.000481\n",
      "Train Epoch: 24\tAverage Loss: 0.000397\n",
      "MAE: 0.015547, RMSE: 0.019934, R2 Score: 0.874610\n",
      "Test Epoch: 24\tAverage Loss: 0.004150\n",
      "MAE: 0.053764, RMSE: 0.064379, R2 Score: -0.291466\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.000361\n",
      "Train Epoch: 25\tAverage Loss: 0.000406\n",
      "MAE: 0.015688, RMSE: 0.020158, R2 Score: 0.871781\n",
      "Test Epoch: 25\tAverage Loss: 0.003861\n",
      "MAE: 0.052144, RMSE: 0.062108, R2 Score: -0.201942\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.000314\n",
      "Train Epoch: 26\tAverage Loss: 0.000409\n",
      "MAE: 0.015920, RMSE: 0.020244, R2 Score: 0.870680\n",
      "Test Epoch: 26\tAverage Loss: 0.004007\n",
      "MAE: 0.053089, RMSE: 0.063297, R2 Score: -0.248418\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.000468\n",
      "Train Epoch: 27\tAverage Loss: 0.000374\n",
      "MAE: 0.015275, RMSE: 0.019333, R2 Score: 0.882051\n",
      "Test Epoch: 27\tAverage Loss: 0.003867\n",
      "MAE: 0.052289, RMSE: 0.062163, R2 Score: -0.204084\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.000319\n",
      "Train Epoch: 28\tAverage Loss: 0.000339\n",
      "MAE: 0.014478, RMSE: 0.018437, R2 Score: 0.892739\n",
      "Test Epoch: 28\tAverage Loss: 0.003989\n",
      "MAE: 0.052929, RMSE: 0.063169, R2 Score: -0.243387\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.000297\n",
      "Train Epoch: 29\tAverage Loss: 0.000342\n",
      "MAE: 0.014316, RMSE: 0.018436, R2 Score: 0.892745\n",
      "Test Epoch: 29\tAverage Loss: 0.003928\n",
      "MAE: 0.052564, RMSE: 0.062653, R2 Score: -0.223140\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.000444\n",
      "Train Epoch: 30\tAverage Loss: 0.000357\n",
      "MAE: 0.014863, RMSE: 0.018909, R2 Score: 0.887169\n",
      "Test Epoch: 30\tAverage Loss: 0.003987\n",
      "MAE: 0.052973, RMSE: 0.063170, R2 Score: -0.243424\n",
      "Fold 3\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.029465\n",
      "Train Epoch: 1\tAverage Loss: 0.006511\n",
      "MAE: 0.062970, RMSE: 0.080782, R2 Score: -1.057997\n",
      "Test Epoch: 1\tAverage Loss: 0.003620\n",
      "MAE: 0.051279, RMSE: 0.060105, R2 Score: -0.128567\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.004173\n",
      "Train Epoch: 2\tAverage Loss: 0.003921\n",
      "MAE: 0.052225, RMSE: 0.062608, R2 Score: -0.236145\n",
      "Test Epoch: 2\tAverage Loss: 0.003578\n",
      "MAE: 0.051132, RMSE: 0.059804, R2 Score: -0.117287\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003557\n",
      "Train Epoch: 3\tAverage Loss: 0.003755\n",
      "MAE: 0.051357, RMSE: 0.061234, R2 Score: -0.182486\n",
      "Test Epoch: 3\tAverage Loss: 0.004254\n",
      "MAE: 0.051700, RMSE: 0.065302, R2 Score: -0.332194\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003278\n",
      "Train Epoch: 4\tAverage Loss: 0.003484\n",
      "MAE: 0.049720, RMSE: 0.059029, R2 Score: -0.098862\n",
      "Test Epoch: 4\tAverage Loss: 0.003401\n",
      "MAE: 0.050119, RMSE: 0.058293, R2 Score: -0.061559\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003191\n",
      "Train Epoch: 5\tAverage Loss: 0.003299\n",
      "MAE: 0.048355, RMSE: 0.057399, R2 Score: -0.038999\n",
      "Test Epoch: 5\tAverage Loss: 0.003586\n",
      "MAE: 0.051123, RMSE: 0.059919, R2 Score: -0.121602\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003015\n",
      "Train Epoch: 6\tAverage Loss: 0.003040\n",
      "MAE: 0.046175, RMSE: 0.055139, R2 Score: 0.041202\n",
      "Test Epoch: 6\tAverage Loss: 0.003651\n",
      "MAE: 0.051855, RMSE: 0.060426, R2 Score: -0.140658\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.002447\n",
      "Train Epoch: 7\tAverage Loss: 0.002729\n",
      "MAE: 0.043304, RMSE: 0.052233, R2 Score: 0.139605\n",
      "Test Epoch: 7\tAverage Loss: 0.003828\n",
      "MAE: 0.052322, RMSE: 0.061870, R2 Score: -0.195810\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002468\n",
      "Train Epoch: 8\tAverage Loss: 0.002330\n",
      "MAE: 0.039885, RMSE: 0.048269, R2 Score: 0.265235\n",
      "Test Epoch: 8\tAverage Loss: 0.004053\n",
      "MAE: 0.053641, RMSE: 0.063590, R2 Score: -0.263233\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.001954\n",
      "Train Epoch: 9\tAverage Loss: 0.001954\n",
      "MAE: 0.035835, RMSE: 0.044146, R2 Score: 0.385392\n",
      "Test Epoch: 9\tAverage Loss: 0.004802\n",
      "MAE: 0.057328, RMSE: 0.069210, R2 Score: -0.496402\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.001337\n",
      "Train Epoch: 10\tAverage Loss: 0.001695\n",
      "MAE: 0.033180, RMSE: 0.041128, R2 Score: 0.466561\n",
      "Test Epoch: 10\tAverage Loss: 0.004473\n",
      "MAE: 0.055810, RMSE: 0.066843, R2 Score: -0.395808\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.001449\n",
      "Train Epoch: 11\tAverage Loss: 0.001336\n",
      "MAE: 0.029174, RMSE: 0.036536, R2 Score: 0.579024\n",
      "Test Epoch: 11\tAverage Loss: 0.004358\n",
      "MAE: 0.055413, RMSE: 0.066028, R2 Score: -0.361962\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.000987\n",
      "Train Epoch: 12\tAverage Loss: 0.001152\n",
      "MAE: 0.026974, RMSE: 0.033944, R2 Score: 0.636631\n",
      "Test Epoch: 12\tAverage Loss: 0.004328\n",
      "MAE: 0.055070, RMSE: 0.065789, R2 Score: -0.352117\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.000873\n",
      "Train Epoch: 13\tAverage Loss: 0.000928\n",
      "MAE: 0.024205, RMSE: 0.030463, R2 Score: 0.707349\n",
      "Test Epoch: 13\tAverage Loss: 0.004465\n",
      "MAE: 0.056033, RMSE: 0.066842, R2 Score: -0.395751\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.001130\n",
      "Train Epoch: 14\tAverage Loss: 0.000809\n",
      "MAE: 0.022480, RMSE: 0.028435, R2 Score: 0.745016\n",
      "Test Epoch: 14\tAverage Loss: 0.004332\n",
      "MAE: 0.055095, RMSE: 0.065748, R2 Score: -0.350450\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.000769\n",
      "Train Epoch: 15\tAverage Loss: 0.000687\n",
      "MAE: 0.020712, RMSE: 0.026184, R2 Score: 0.783781\n",
      "Test Epoch: 15\tAverage Loss: 0.004231\n",
      "MAE: 0.054670, RMSE: 0.065018, R2 Score: -0.320604\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.000711\n",
      "Train Epoch: 16\tAverage Loss: 0.000632\n",
      "MAE: 0.019974, RMSE: 0.025127, R2 Score: 0.800897\n",
      "Test Epoch: 16\tAverage Loss: 0.004232\n",
      "MAE: 0.054796, RMSE: 0.065059, R2 Score: -0.322296\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.000560\n",
      "Train Epoch: 17\tAverage Loss: 0.000574\n",
      "MAE: 0.018932, RMSE: 0.023952, R2 Score: 0.819082\n",
      "Test Epoch: 17\tAverage Loss: 0.004169\n",
      "MAE: 0.054375, RMSE: 0.064543, R2 Score: -0.301381\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.000542\n",
      "Train Epoch: 18\tAverage Loss: 0.000526\n",
      "MAE: 0.018152, RMSE: 0.022923, R2 Score: 0.834287\n",
      "Test Epoch: 18\tAverage Loss: 0.004136\n",
      "MAE: 0.054109, RMSE: 0.064253, R2 Score: -0.289706\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.000362\n",
      "Train Epoch: 19\tAverage Loss: 0.000482\n",
      "MAE: 0.017311, RMSE: 0.021934, R2 Score: 0.848280\n",
      "Test Epoch: 19\tAverage Loss: 0.004185\n",
      "MAE: 0.054362, RMSE: 0.064622, R2 Score: -0.304591\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.000379\n",
      "Train Epoch: 20\tAverage Loss: 0.000460\n",
      "MAE: 0.016913, RMSE: 0.021437, R2 Score: 0.855079\n",
      "Test Epoch: 20\tAverage Loss: 0.004036\n",
      "MAE: 0.053791, RMSE: 0.063516, R2 Score: -0.260301\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.000316\n",
      "Train Epoch: 21\tAverage Loss: 0.000424\n",
      "MAE: 0.016124, RMSE: 0.020519, R2 Score: 0.867223\n",
      "Test Epoch: 21\tAverage Loss: 0.004071\n",
      "MAE: 0.053986, RMSE: 0.063824, R2 Score: -0.272536\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.000426\n",
      "Train Epoch: 22\tAverage Loss: 0.000414\n",
      "MAE: 0.016047, RMSE: 0.020347, R2 Score: 0.869445\n",
      "Test Epoch: 22\tAverage Loss: 0.004046\n",
      "MAE: 0.053992, RMSE: 0.063610, R2 Score: -0.264055\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.000329\n",
      "Train Epoch: 23\tAverage Loss: 0.000394\n",
      "MAE: 0.015656, RMSE: 0.019853, R2 Score: 0.875702\n",
      "Test Epoch: 23\tAverage Loss: 0.004194\n",
      "MAE: 0.054483, RMSE: 0.064739, R2 Score: -0.309312\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.000539\n",
      "Train Epoch: 24\tAverage Loss: 0.000387\n",
      "MAE: 0.015457, RMSE: 0.019675, R2 Score: 0.877920\n",
      "Test Epoch: 24\tAverage Loss: 0.004042\n",
      "MAE: 0.053716, RMSE: 0.063506, R2 Score: -0.259919\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.000447\n",
      "Train Epoch: 25\tAverage Loss: 0.000386\n",
      "MAE: 0.015396, RMSE: 0.019653, R2 Score: 0.878188\n",
      "Test Epoch: 25\tAverage Loss: 0.003993\n",
      "MAE: 0.053421, RMSE: 0.063099, R2 Score: -0.243825\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.000410\n",
      "Train Epoch: 26\tAverage Loss: 0.000358\n",
      "MAE: 0.014961, RMSE: 0.018884, R2 Score: 0.887535\n",
      "Test Epoch: 26\tAverage Loss: 0.003922\n",
      "MAE: 0.053044, RMSE: 0.062579, R2 Score: -0.223411\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.000366\n",
      "Train Epoch: 27\tAverage Loss: 0.000364\n",
      "MAE: 0.014946, RMSE: 0.019059, R2 Score: 0.885450\n",
      "Test Epoch: 27\tAverage Loss: 0.004021\n",
      "MAE: 0.053599, RMSE: 0.063371, R2 Score: -0.254538\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.000319\n",
      "Train Epoch: 28\tAverage Loss: 0.000347\n",
      "MAE: 0.014644, RMSE: 0.018621, R2 Score: 0.890655\n",
      "Test Epoch: 28\tAverage Loss: 0.003951\n",
      "MAE: 0.053193, RMSE: 0.062815, R2 Score: -0.232632\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.000509\n",
      "Train Epoch: 29\tAverage Loss: 0.000336\n",
      "MAE: 0.014372, RMSE: 0.018338, R2 Score: 0.893954\n",
      "Test Epoch: 29\tAverage Loss: 0.004071\n",
      "MAE: 0.053889, RMSE: 0.063772, R2 Score: -0.270489\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.000344\n",
      "Train Epoch: 30\tAverage Loss: 0.000321\n",
      "MAE: 0.014007, RMSE: 0.017907, R2 Score: 0.898871\n",
      "Test Epoch: 30\tAverage Loss: 0.003930\n",
      "MAE: 0.053233, RMSE: 0.062687, R2 Score: -0.227624\n",
      "Fold 4\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.012672\n",
      "Train Epoch: 1\tAverage Loss: 0.005682\n",
      "MAE: 0.057083, RMSE: 0.075461, R2 Score: -0.774707\n",
      "Test Epoch: 1\tAverage Loss: 0.003254\n",
      "MAE: 0.048990, RMSE: 0.057209, R2 Score: -0.072656\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.003529\n",
      "Train Epoch: 2\tAverage Loss: 0.003391\n",
      "MAE: 0.049629, RMSE: 0.058259, R2 Score: -0.057815\n",
      "Test Epoch: 2\tAverage Loss: 0.003118\n",
      "MAE: 0.047918, RMSE: 0.056032, R2 Score: -0.028982\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.003926\n",
      "Train Epoch: 3\tAverage Loss: 0.003343\n",
      "MAE: 0.049414, RMSE: 0.057841, R2 Score: -0.042709\n",
      "Test Epoch: 3\tAverage Loss: 0.003126\n",
      "MAE: 0.048134, RMSE: 0.056078, R2 Score: -0.030673\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.002952\n",
      "Train Epoch: 4\tAverage Loss: 0.003287\n",
      "MAE: 0.049067, RMSE: 0.057303, R2 Score: -0.023382\n",
      "Test Epoch: 4\tAverage Loss: 0.003134\n",
      "MAE: 0.048144, RMSE: 0.056144, R2 Score: -0.033116\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.002763\n",
      "Train Epoch: 5\tAverage Loss: 0.003149\n",
      "MAE: 0.047969, RMSE: 0.056147, R2 Score: 0.017503\n",
      "Test Epoch: 5\tAverage Loss: 0.003134\n",
      "MAE: 0.048210, RMSE: 0.056158, R2 Score: -0.033626\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.003234\n",
      "Train Epoch: 6\tAverage Loss: 0.002991\n",
      "MAE: 0.046599, RMSE: 0.054741, R2 Score: 0.066089\n",
      "Test Epoch: 6\tAverage Loss: 0.003244\n",
      "MAE: 0.048869, RMSE: 0.057116, R2 Score: -0.069187\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.002987\n",
      "Train Epoch: 7\tAverage Loss: 0.002732\n",
      "MAE: 0.043973, RMSE: 0.052272, R2 Score: 0.148439\n",
      "Test Epoch: 7\tAverage Loss: 0.003341\n",
      "MAE: 0.049418, RMSE: 0.057947, R2 Score: -0.100518\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002359\n",
      "Train Epoch: 8\tAverage Loss: 0.002436\n",
      "MAE: 0.041007, RMSE: 0.049357, R2 Score: 0.240745\n",
      "Test Epoch: 8\tAverage Loss: 0.003595\n",
      "MAE: 0.050765, RMSE: 0.060154, R2 Score: -0.185962\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.002140\n",
      "Train Epoch: 9\tAverage Loss: 0.002042\n",
      "MAE: 0.036970, RMSE: 0.045197, R2 Score: 0.363342\n",
      "Test Epoch: 9\tAverage Loss: 0.003558\n",
      "MAE: 0.050594, RMSE: 0.059788, R2 Score: -0.171554\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.001705\n",
      "Train Epoch: 10\tAverage Loss: 0.001696\n",
      "MAE: 0.033136, RMSE: 0.041217, R2 Score: 0.470530\n",
      "Test Epoch: 10\tAverage Loss: 0.004254\n",
      "MAE: 0.054285, RMSE: 0.065349, R2 Score: -0.399647\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.001643\n",
      "Train Epoch: 11\tAverage Loss: 0.001358\n",
      "MAE: 0.029407, RMSE: 0.036851, R2 Score: 0.576753\n",
      "Test Epoch: 11\tAverage Loss: 0.004326\n",
      "MAE: 0.055164, RMSE: 0.065989, R2 Score: -0.427182\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.000917\n",
      "Train Epoch: 12\tAverage Loss: 0.001148\n",
      "MAE: 0.026774, RMSE: 0.033883, R2 Score: 0.642196\n",
      "Test Epoch: 12\tAverage Loss: 0.004274\n",
      "MAE: 0.054292, RMSE: 0.065496, R2 Score: -0.405936\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.001001\n",
      "Train Epoch: 13\tAverage Loss: 0.000984\n",
      "MAE: 0.024912, RMSE: 0.031358, R2 Score: 0.693525\n",
      "Test Epoch: 13\tAverage Loss: 0.003975\n",
      "MAE: 0.052976, RMSE: 0.063131, R2 Score: -0.306225\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.000638\n",
      "Train Epoch: 14\tAverage Loss: 0.000838\n",
      "MAE: 0.022893, RMSE: 0.028941, R2 Score: 0.738955\n",
      "Test Epoch: 14\tAverage Loss: 0.003813\n",
      "MAE: 0.051834, RMSE: 0.061839, R2 Score: -0.253318\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.000704\n",
      "Train Epoch: 15\tAverage Loss: 0.000711\n",
      "MAE: 0.021209, RMSE: 0.026646, R2 Score: 0.778715\n",
      "Test Epoch: 15\tAverage Loss: 0.004043\n",
      "MAE: 0.053208, RMSE: 0.063713, R2 Score: -0.330419\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.000510\n",
      "Train Epoch: 16\tAverage Loss: 0.000645\n",
      "MAE: 0.020183, RMSE: 0.025388, R2 Score: 0.799116\n",
      "Test Epoch: 16\tAverage Loss: 0.003907\n",
      "MAE: 0.052726, RMSE: 0.062627, R2 Score: -0.285459\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.000517\n",
      "Train Epoch: 17\tAverage Loss: 0.000559\n",
      "MAE: 0.018714, RMSE: 0.023661, R2 Score: 0.825510\n",
      "Test Epoch: 17\tAverage Loss: 0.003962\n",
      "MAE: 0.053008, RMSE: 0.063018, R2 Score: -0.301578\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.000448\n",
      "Train Epoch: 18\tAverage Loss: 0.000529\n",
      "MAE: 0.018251, RMSE: 0.022985, R2 Score: 0.835341\n",
      "Test Epoch: 18\tAverage Loss: 0.003883\n",
      "MAE: 0.052215, RMSE: 0.062409, R2 Score: -0.276540\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.000472\n",
      "Train Epoch: 19\tAverage Loss: 0.000511\n",
      "MAE: 0.017769, RMSE: 0.022604, R2 Score: 0.840754\n",
      "Test Epoch: 19\tAverage Loss: 0.004090\n",
      "MAE: 0.053559, RMSE: 0.064108, R2 Score: -0.346973\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.000517\n",
      "Train Epoch: 20\tAverage Loss: 0.000516\n",
      "MAE: 0.017880, RMSE: 0.022683, R2 Score: 0.839645\n",
      "Test Epoch: 20\tAverage Loss: 0.004376\n",
      "MAE: 0.055406, RMSE: 0.066248, R2 Score: -0.438415\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.000554\n",
      "Train Epoch: 21\tAverage Loss: 0.000461\n",
      "MAE: 0.016871, RMSE: 0.021457, R2 Score: 0.856505\n",
      "Test Epoch: 21\tAverage Loss: 0.003870\n",
      "MAE: 0.052236, RMSE: 0.062314, R2 Score: -0.272648\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.000475\n",
      "Train Epoch: 22\tAverage Loss: 0.000417\n",
      "MAE: 0.016004, RMSE: 0.020410, R2 Score: 0.870167\n",
      "Test Epoch: 22\tAverage Loss: 0.003975\n",
      "MAE: 0.052712, RMSE: 0.063147, R2 Score: -0.306920\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.000528\n",
      "Train Epoch: 23\tAverage Loss: 0.000409\n",
      "MAE: 0.015907, RMSE: 0.020204, R2 Score: 0.872775\n",
      "Test Epoch: 23\tAverage Loss: 0.003957\n",
      "MAE: 0.052619, RMSE: 0.063008, R2 Score: -0.301142\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.000408\n",
      "Train Epoch: 24\tAverage Loss: 0.000403\n",
      "MAE: 0.015736, RMSE: 0.020044, R2 Score: 0.874791\n",
      "Test Epoch: 24\tAverage Loss: 0.004108\n",
      "MAE: 0.053427, RMSE: 0.064218, R2 Score: -0.351615\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.000359\n",
      "Train Epoch: 25\tAverage Loss: 0.000406\n",
      "MAE: 0.015758, RMSE: 0.020093, R2 Score: 0.874176\n",
      "Test Epoch: 25\tAverage Loss: 0.003872\n",
      "MAE: 0.052351, RMSE: 0.062330, R2 Score: -0.273310\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.000406\n",
      "Train Epoch: 26\tAverage Loss: 0.000380\n",
      "MAE: 0.015332, RMSE: 0.019513, R2 Score: 0.881336\n",
      "Test Epoch: 26\tAverage Loss: 0.003806\n",
      "MAE: 0.051714, RMSE: 0.061796, R2 Score: -0.251591\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.000257\n",
      "Train Epoch: 27\tAverage Loss: 0.000351\n",
      "MAE: 0.014704, RMSE: 0.018722, R2 Score: 0.890758\n",
      "Test Epoch: 27\tAverage Loss: 0.003922\n",
      "MAE: 0.052615, RMSE: 0.062741, R2 Score: -0.290163\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.000282\n",
      "Train Epoch: 28\tAverage Loss: 0.000341\n",
      "MAE: 0.014430, RMSE: 0.018448, R2 Score: 0.893936\n",
      "Test Epoch: 28\tAverage Loss: 0.003781\n",
      "MAE: 0.051807, RMSE: 0.061595, R2 Score: -0.243430\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.000335\n",
      "Train Epoch: 29\tAverage Loss: 0.000345\n",
      "MAE: 0.014567, RMSE: 0.018538, R2 Score: 0.892889\n",
      "Test Epoch: 29\tAverage Loss: 0.003871\n",
      "MAE: 0.052237, RMSE: 0.062374, R2 Score: -0.275102\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.000314\n",
      "Train Epoch: 30\tAverage Loss: 0.000317\n",
      "MAE: 0.014021, RMSE: 0.017833, R2 Score: 0.900891\n",
      "Test Epoch: 30\tAverage Loss: 0.003956\n",
      "MAE: 0.052451, RMSE: 0.062981, R2 Score: -0.300052\n",
      "Fold 5\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.035719\n",
      "Train Epoch: 1\tAverage Loss: 0.007724\n",
      "MAE: 0.065873, RMSE: 0.088082, R2 Score: -1.444381\n",
      "Test Epoch: 1\tAverage Loss: 0.003645\n",
      "MAE: 0.051321, RMSE: 0.060181, R2 Score: -0.136152\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.004571\n",
      "Train Epoch: 2\tAverage Loss: 0.004236\n",
      "MAE: 0.054081, RMSE: 0.065059, R2 Score: -0.333533\n",
      "Test Epoch: 2\tAverage Loss: 0.003612\n",
      "MAE: 0.050752, RMSE: 0.060091, R2 Score: -0.132737\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.004274\n",
      "Train Epoch: 3\tAverage Loss: 0.003866\n",
      "MAE: 0.051885, RMSE: 0.062244, R2 Score: -0.220650\n",
      "Test Epoch: 3\tAverage Loss: 0.003535\n",
      "MAE: 0.050324, RMSE: 0.059336, R2 Score: -0.104445\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.003682\n",
      "Train Epoch: 4\tAverage Loss: 0.003644\n",
      "MAE: 0.050580, RMSE: 0.060351, R2 Score: -0.147516\n",
      "Test Epoch: 4\tAverage Loss: 0.003630\n",
      "MAE: 0.050705, RMSE: 0.060167, R2 Score: -0.135592\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.003903\n",
      "Train Epoch: 5\tAverage Loss: 0.003379\n",
      "MAE: 0.048815, RMSE: 0.058095, R2 Score: -0.063321\n",
      "Test Epoch: 5\tAverage Loss: 0.003512\n",
      "MAE: 0.050414, RMSE: 0.059330, R2 Score: -0.104220\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.002221\n",
      "Train Epoch: 6\tAverage Loss: 0.003119\n",
      "MAE: 0.046698, RMSE: 0.055818, R2 Score: 0.018373\n",
      "Test Epoch: 6\tAverage Loss: 0.003796\n",
      "MAE: 0.051817, RMSE: 0.061609, R2 Score: -0.190677\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.003107\n",
      "Train Epoch: 7\tAverage Loss: 0.002754\n",
      "MAE: 0.043330, RMSE: 0.052520, R2 Score: 0.130972\n",
      "Test Epoch: 7\tAverage Loss: 0.003759\n",
      "MAE: 0.052012, RMSE: 0.061335, R2 Score: -0.180128\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.002274\n",
      "Train Epoch: 8\tAverage Loss: 0.002345\n",
      "MAE: 0.039763, RMSE: 0.048446, R2 Score: 0.260558\n",
      "Test Epoch: 8\tAverage Loss: 0.004049\n",
      "MAE: 0.053328, RMSE: 0.063649, R2 Score: -0.270864\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.001604\n",
      "Train Epoch: 9\tAverage Loss: 0.002073\n",
      "MAE: 0.036923, RMSE: 0.045534, R2 Score: 0.346773\n",
      "Test Epoch: 9\tAverage Loss: 0.003943\n",
      "MAE: 0.052452, RMSE: 0.062849, R2 Score: -0.239115\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.001929\n",
      "Train Epoch: 10\tAverage Loss: 0.001651\n",
      "MAE: 0.032702, RMSE: 0.040674, R2 Score: 0.478785\n",
      "Test Epoch: 10\tAverage Loss: 0.004342\n",
      "MAE: 0.055231, RMSE: 0.065849, R2 Score: -0.360238\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.001804\n",
      "Train Epoch: 11\tAverage Loss: 0.001333\n",
      "MAE: 0.028874, RMSE: 0.036503, R2 Score: 0.580201\n",
      "Test Epoch: 11\tAverage Loss: 0.004231\n",
      "MAE: 0.054390, RMSE: 0.065105, R2 Score: -0.329658\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.001159\n",
      "Train Epoch: 12\tAverage Loss: 0.001150\n",
      "MAE: 0.026817, RMSE: 0.033910, R2 Score: 0.637726\n",
      "Test Epoch: 12\tAverage Loss: 0.004233\n",
      "MAE: 0.054401, RMSE: 0.065011, R2 Score: -0.325813\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.000821\n",
      "Train Epoch: 13\tAverage Loss: 0.000961\n",
      "MAE: 0.024399, RMSE: 0.031015, R2 Score: 0.696939\n",
      "Test Epoch: 13\tAverage Loss: 0.004377\n",
      "MAE: 0.055268, RMSE: 0.066157, R2 Score: -0.372989\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.000807\n",
      "Train Epoch: 14\tAverage Loss: 0.000834\n",
      "MAE: 0.022852, RMSE: 0.028862, R2 Score: 0.737558\n",
      "Test Epoch: 14\tAverage Loss: 0.004463\n",
      "MAE: 0.055846, RMSE: 0.066830, R2 Score: -0.401045\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.000649\n",
      "Train Epoch: 15\tAverage Loss: 0.000741\n",
      "MAE: 0.021427, RMSE: 0.027194, R2 Score: 0.767011\n",
      "Test Epoch: 15\tAverage Loss: 0.004359\n",
      "MAE: 0.055084, RMSE: 0.066012, R2 Score: -0.366959\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.000598\n",
      "Train Epoch: 16\tAverage Loss: 0.000645\n",
      "MAE: 0.020011, RMSE: 0.025395, R2 Score: 0.796820\n",
      "Test Epoch: 16\tAverage Loss: 0.004283\n",
      "MAE: 0.054564, RMSE: 0.065419, R2 Score: -0.342532\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.000563\n",
      "Train Epoch: 17\tAverage Loss: 0.000574\n",
      "MAE: 0.018897, RMSE: 0.023955, R2 Score: 0.819205\n",
      "Test Epoch: 17\tAverage Loss: 0.004158\n",
      "MAE: 0.053867, RMSE: 0.064398, R2 Score: -0.300932\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.000439\n",
      "Train Epoch: 18\tAverage Loss: 0.000507\n",
      "MAE: 0.017703, RMSE: 0.022519, R2 Score: 0.840238\n",
      "Test Epoch: 18\tAverage Loss: 0.004143\n",
      "MAE: 0.053729, RMSE: 0.064285, R2 Score: -0.296392\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.000349\n",
      "Train Epoch: 19\tAverage Loss: 0.000499\n",
      "MAE: 0.017534, RMSE: 0.022338, R2 Score: 0.842791\n",
      "Test Epoch: 19\tAverage Loss: 0.004092\n",
      "MAE: 0.053632, RMSE: 0.063894, R2 Score: -0.280670\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.000341\n",
      "Train Epoch: 20\tAverage Loss: 0.000475\n",
      "MAE: 0.017112, RMSE: 0.021715, R2 Score: 0.851441\n",
      "Test Epoch: 20\tAverage Loss: 0.004074\n",
      "MAE: 0.053757, RMSE: 0.063799, R2 Score: -0.276834\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.000403\n",
      "Train Epoch: 21\tAverage Loss: 0.000461\n",
      "MAE: 0.016891, RMSE: 0.021454, R2 Score: 0.854991\n",
      "Test Epoch: 21\tAverage Loss: 0.003935\n",
      "MAE: 0.052735, RMSE: 0.062680, R2 Score: -0.232466\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.000410\n",
      "Train Epoch: 22\tAverage Loss: 0.000439\n",
      "MAE: 0.016473, RMSE: 0.020934, R2 Score: 0.861928\n",
      "Test Epoch: 22\tAverage Loss: 0.003844\n",
      "MAE: 0.052236, RMSE: 0.062025, R2 Score: -0.206813\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.000361\n",
      "Train Epoch: 23\tAverage Loss: 0.000403\n",
      "MAE: 0.015752, RMSE: 0.020045, R2 Score: 0.873414\n",
      "Test Epoch: 23\tAverage Loss: 0.004084\n",
      "MAE: 0.053331, RMSE: 0.063879, R2 Score: -0.280045\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.000367\n",
      "Train Epoch: 24\tAverage Loss: 0.000406\n",
      "MAE: 0.015843, RMSE: 0.020154, R2 Score: 0.872023\n",
      "Test Epoch: 24\tAverage Loss: 0.004027\n",
      "MAE: 0.053144, RMSE: 0.063427, R2 Score: -0.262009\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.000347\n",
      "Train Epoch: 25\tAverage Loss: 0.000372\n",
      "MAE: 0.015270, RMSE: 0.019303, R2 Score: 0.882611\n",
      "Test Epoch: 25\tAverage Loss: 0.003988\n",
      "MAE: 0.053170, RMSE: 0.063130, R2 Score: -0.250204\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.000402\n",
      "Train Epoch: 26\tAverage Loss: 0.000368\n",
      "MAE: 0.015091, RMSE: 0.019186, R2 Score: 0.884027\n",
      "Test Epoch: 26\tAverage Loss: 0.004072\n",
      "MAE: 0.053545, RMSE: 0.063811, R2 Score: -0.277320\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.000344\n",
      "Train Epoch: 27\tAverage Loss: 0.000371\n",
      "MAE: 0.015169, RMSE: 0.019248, R2 Score: 0.883274\n",
      "Test Epoch: 27\tAverage Loss: 0.004142\n",
      "MAE: 0.053735, RMSE: 0.064413, R2 Score: -0.301535\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.000362\n",
      "Train Epoch: 28\tAverage Loss: 0.000335\n",
      "MAE: 0.014349, RMSE: 0.018273, R2 Score: 0.894799\n",
      "Test Epoch: 28\tAverage Loss: 0.003987\n",
      "MAE: 0.052774, RMSE: 0.063036, R2 Score: -0.246505\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.000414\n",
      "Train Epoch: 29\tAverage Loss: 0.000347\n",
      "MAE: 0.014611, RMSE: 0.018607, R2 Score: 0.890921\n",
      "Test Epoch: 29\tAverage Loss: 0.003968\n",
      "MAE: 0.052909, RMSE: 0.062959, R2 Score: -0.243457\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.000303\n",
      "Train Epoch: 30\tAverage Loss: 0.000338\n",
      "MAE: 0.014480, RMSE: 0.018413, R2 Score: 0.893184\n",
      "Test Epoch: 30\tAverage Loss: 0.003966\n",
      "MAE: 0.052793, RMSE: 0.062995, R2 Score: -0.244875\n"
     ]
    }
   ],
   "source": [
    "#Efficientnet\n",
    "\n",
    "from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "# Initialize empty lists to store metrics\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Training and testing loop with cross-validation\n",
    "epochs = 30\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    trainloader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "    testloader = DataLoader(test_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = NoiseNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    custom_run_name = f\"Effc_fold_{fold + 1}\"\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{custom_run_name}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, trainloader, loss_fn, optimizer, epoch, writer)\n",
    "        test(model, testloader, loss_fn, epoch, writer)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Read scalar values from tensorboard event files\n",
    "\n",
    "    event_file = glob.glob(f\"runs/{custom_run_name}/events.out.tfevents.*\")[0]\n",
    "    event_loader = EventFileLoader(event_file)\n",
    "    scalar_events = [event for event in event_loader.Load()]\n",
    "\n",
    "\n",
    "    # Extract the required scalar values\n",
    "    train_loss = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 1), None)\n",
    "    test_loss = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 2), None)\n",
    "    mae = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 3), None)\n",
    "    rmse = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 4), None)\n",
    "    r2 = next((event.summary.value[0].simple_value for event in scalar_events if event.step == 5), None)\n",
    "\n",
    "\n",
    "    # Append the metrics to the respective lists\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3567f7-9449-4490-ad6a-01f2eabb8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dc068-7024-4b79-9b9a-d5fe08cd3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 13784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0ae483-9a36-4df7-8fdc-51f24aa15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7713034f-08bc-4fd4-af41-f21bb4cdca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a5bc83217c0e1059\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a5bc83217c0e1059\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "logs_base_dir = \"./runs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836598d5-82a6-4a27-8390-00174d519295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
